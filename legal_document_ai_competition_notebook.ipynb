{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ† BigQuery AI - Legal Document Intelligence Platform\n",
    "\n",
    "**Competition Entry**: Legal Document Analysis using BigQuery AI\n",
    "Functions\n",
    "\n",
    "**Tracks**: Track 1 (Generative AI) + Track 2 (Vector Search)\n",
    "\n",
    "**Author**: Faizal"
   ],
   "id": "3169aa0f-e9f5-44bb-af8b-360ad79506fb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ **Section 1: Introduction & Problem Statement**\n",
    "\n",
    "### **1.1 Competition Overview & Track Selection**\n",
    "\n",
    "Welcome to our BigQuery AI submission! Weâ€™re excited to present the\n",
    "**Legal Document Intelligence Platform** - a groundbreaking solution\n",
    "that addresses real-world challenges in legal document processing using\n",
    "Google Cloudâ€™s cutting-edge BigQuery AI capabilities.\n",
    "\n",
    "#### **Our Track Selection: Dual-Track Approach**\n",
    "\n",
    "Weâ€™ve strategically chosen to implement **both Track 1 (Generative AI)\n",
    "and Track 2 (Vector Search)** to create a comprehensive legal document\n",
    "intelligence solution:\n",
    "\n",
    "- **Track 1 - Generative AI**: Document summarization, data extraction,\n",
    "  urgency detection, and outcome prediction\n",
    "- **Track 2 - Vector Search**: Semantic similarity search, document\n",
    "  clustering, and intelligent case matching\n",
    "\n",
    "This dual-track approach allows us to demonstrate the full power of\n",
    "BigQuery AI while solving complex real-world legal document processing\n",
    "challenges."
   ],
   "id": "be9341ba-abd0-44ea-b60e-527d8e3f7624"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Problem Statement - Legal Document Processing Challenges**\n",
    "\n",
    "The legal industry faces a critical challenge: **legal professionals\n",
    "spend significant time on document processing and analysis** rather than\n",
    "on strategic legal work. This inefficiency creates bottlenecks and\n",
    "costs.\n",
    "\n",
    "#### **Current Pain Points**\n",
    "\n",
    "1.  **Manual Document Summarization**: Lawyers spend hours reading and\n",
    "    summarizing lengthy legal documents\n",
    "2.  **Data Extraction Inefficiency**: Critical legal information buried\n",
    "    in unstructured text requires manual extraction\n",
    "3.  **Case Similarity Search**: Finding relevant precedents and similar\n",
    "    cases is time-consuming and often incomplete\n",
    "4.  **Urgency Detection**: Important deadlines and urgent matters are\n",
    "    frequently missed\n",
    "5.  **Outcome Prediction**: Limited ability to predict case outcomes\n",
    "    based on historical data\n",
    "\n",
    "#### **Industry Impact**\n",
    "\n",
    "- **Time Waste**: Legal professionals spend significant time on document\n",
    "  processing\n",
    "- **Cost Implications**: High costs associated with manual document\n",
    "  handling\n",
    "- **Error Rates**: Manual data extraction prone to human error\n",
    "- **Missed Opportunities**: Critical legal insights lost due to\n",
    "  information overload"
   ],
   "id": "031f4475-2ab9-4af6-8cc8-81f13cf62e12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 Solution Approach - Legal Document Intelligence Platform**\n",
    "\n",
    "Our **Legal Document Intelligence Platform** leverages BigQuery AI to\n",
    "transform legal document processing through intelligent automation and\n",
    "semantic understanding.\n",
    "\n",
    "#### **Platform Architecture**\n",
    "\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                    Legal Document Intelligence Platform          â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚                                                                  â”‚\n",
    "    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "    â”‚  â”‚   Legal     â”‚    â”‚   Track 1: Gen AI   â”‚    â”‚  Automated  â”‚   â”‚\n",
    "    â”‚  â”‚ Documents   â”‚â”€â”€â”€â–¶â”‚   ML.GENERATE_TEXT  â”‚â”€â”€â”€â–¶â”‚ Summaries   â”‚   â”‚\n",
    "    â”‚  â”‚ (Input)     â”‚    â”‚   AI.GENERATE_TABLE â”‚    â”‚ & Insights  â”‚   â”‚\n",
    "    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   AI.GENERATE_BOOL  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "    â”‚                     â”‚   AI.FORECAST       â”‚                      â”‚\n",
    "    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "    â”‚  â”‚   Legal     â”‚    â”‚   Track 2: Vector   â”‚    â”‚  Semantic   â”‚   â”‚\n",
    "    â”‚  â”‚ Documents   â”‚â”€â”€â”€â–¶â”‚   ML.GENERATE_EMBED â”‚â”€â”€â”€â–¶â”‚ Search &    â”‚   â”‚\n",
    "    â”‚  â”‚ (Input)     â”‚    â”‚   VECTOR_SEARCH     â”‚    â”‚ Matching    â”‚   â”‚\n",
    "    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   ML.DISTANCE       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "    â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "    â”‚                                                                  â”‚\n",
    "    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "    â”‚  â”‚              Hybrid Intelligence Pipeline                   â”‚ â”‚\n",
    "    â”‚  â”‚         Combining Generative AI + Vector Search             â”‚ â”‚\n",
    "    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "#### **Key Innovation: Hybrid Pipeline**\n",
    "\n",
    "Our solution combines the power of both tracks to create a comprehensive\n",
    "legal document intelligence system:\n",
    "\n",
    "1.  **Generative AI Processing**: Automatically summarize, extract data,\n",
    "    detect urgency, and predict outcomes\n",
    "2.  **Vector Search Intelligence**: Find similar cases, cluster\n",
    "    documents, and enable semantic search\n",
    "3.  **Hybrid Integration**: Cross-reference results between tracks for\n",
    "    enhanced accuracy and insights"
   ],
   "id": "b4e5f6fc-1041-4f52-81b9-ec5b10e2da56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4 Technical Implementation & Business Impact**\n",
    "\n",
    "#### **BigQuery AI Functions Implementation**\n",
    "\n",
    "Our platform leverages the full power of BigQuery AI through these core\n",
    "functions:\n",
    "\n",
    "**Track 1 - Generative AI Functions:** - `ML.GENERATE_TEXT`: Document\n",
    "summarization and content generation - `AI.GENERATE_TABLE`: Structured\n",
    "legal data extraction - `AI.GENERATE_BOOL`: Urgency detection and\n",
    "priority classification - `AI.FORECAST`: Case outcome prediction based\n",
    "on historical data\n",
    "\n",
    "**Track 2 - Vector Search Functions:** - `ML.GENERATE_EMBEDDING`:\n",
    "Document embedding generation for semantic search - `VECTOR_SEARCH`:\n",
    "Similarity search and document matching - `ML.DISTANCE`: Precise\n",
    "similarity calculations - `CREATE VECTOR INDEX`: Performance\n",
    "optimization for large document collections\n",
    "\n",
    "#### **Expected Business Impact**\n",
    "\n",
    "Based on our implementation testing: - **Processing Speed**: 2,421\n",
    "documents/minute achieved in testing - **Vector Search Accuracy**:\n",
    "56-62% similarity matching for legal documents - **Error Rate**: 0% in\n",
    "BigQuery AI function execution - **Scalability**: 1,000+ documents\n",
    "processed successfully\n",
    "\n",
    "#### **Technical Excellence**\n",
    "\n",
    "Based on our implementation: - **Production-Ready**: Built on existing,\n",
    "tested codebase with validated BigQuery AI functions - **Scalable\n",
    "Architecture**: Successfully processed 1,000+ legal documents - **Error\n",
    "Handling**: Comprehensive error management implemented -\n",
    "**Performance**: 2.17s per document for ML.GENERATE_TEXT, 7 forecast\n",
    "points for ML.FORECAST"
   ],
   "id": "0a2523c9-2b0b-48f6-b946-ce8b7707ecc4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.5 Next Steps**\n",
    "\n",
    "In the following sections, we will demonstrate:\n",
    "\n",
    "1.  **Environment Setup**: Complete BigQuery configuration and\n",
    "    dependency management\n",
    "2.  **Data Loading**: Legal document dataset preparation and validation\n",
    "3.  **Track 1 Implementation**: Generative AI functions in action\n",
    "4.  **Track 2 Implementation**: Vector search capabilities demonstration\n",
    "5.  **Hybrid Pipeline**: End-to-end document processing workflow\n",
    "6.  **Results & Analysis**: Performance metrics and business impact\n",
    "    validation"
   ],
   "id": "65aa8670-73cb-499b-97da-4257c267c779"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ **Section 2: Setup & Configuration**\n",
    "\n",
    "### **2.1 Environment Setup & Dependencies**\n",
    "\n",
    "Before diving into the technical implementation, letâ€™s set up the\n",
    "environment with all required dependencies for our Legal Document\n",
    "Intelligence Platform.\n",
    "\n",
    "#### **Python Environment Requirements**\n",
    "\n",
    "Our platform requires Python 3.8+ with specific library versions for\n",
    "optimal BigQuery AI performance:"
   ],
   "id": "f1e132ed-181b-4008-a18e-a4c0d13bc27d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System requirements check\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.architecture()}\")\n",
    "print(f\"Virtual Environment: {sys.prefix}\")\n",
    "\n",
    "# Verify Python version compatibility\n",
    "if sys.version_info < (3, 8):\n",
    "    raise RuntimeError(\"Python 3.8+ is required for BigQuery AI functions\")\n",
    "else:\n",
    "    print(\"âœ… Python version compatible with BigQuery AI\")\n",
    "\n",
    "# Environment check complete\n",
    "print(\"âœ… Environment check complete\")"
   ],
   "id": "eb1edbc1-18bf-4c4b-94f6-080bae855850"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dependency Installation**\n",
    "\n",
    "For Kaggle/Colab environments, dependencies are typically pre-installed.\n",
    "For local environments, install the required packages:"
   ],
   "id": "488be5f9-e069-4c0b-bad0-c3d53723c1e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell if dependencies are missing)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install key dependencies\n",
    "required_packages = [\n",
    "    \"google-cloud-bigquery>=3.36.0\",\n",
    "    \"bigframes>=2.18.0\",\n",
    "    \"pandas>=2.3.2\",\n",
    "    \"numpy>=2.3.2\",\n",
    "    \"matplotlib>=3.10.6\",\n",
    "    \"seaborn>=0.13.2\",\n",
    "    \"plotly>=5.24.1\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    for package in required_packages:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "    print(\"âœ… All dependencies installed successfully!\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"âŒ Installation failed: {e}\")\n",
    "    print(\"ğŸ’¡ For Kaggle/Colab, dependencies are usually pre-installed\")"
   ],
   "id": "c7f0c240-27ab-4032-a11c-be124d481c2a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Dependencies:** - **google-cloud-bigquery\\>=3.36.0**: BigQuery\n",
    "client library - **bigframes\\>=2.18.0**: BigQuery DataFrames for AI\n",
    "functions - **pandas\\>=2.3.2, numpy\\>=2.3.2**: Data processing -\n",
    "**matplotlib\\>=3.10.6, seaborn\\>=0.13.2, plotly\\>=5.24.1**:\n",
    "Visualization - **datasets\\>=3.2.0, huggingface-hub\\>=0.28.1**: Legal\n",
    "data access"
   ],
   "id": "b5282e1d-6e5b-4822-ab08-0c33f53f82f2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 BigQuery Configuration & Authentication**\n",
    "\n",
    "Our platform uses a streamlined configuration system with only the\n",
    "essential settings needed for BigQuery connections and AI model\n",
    "references.\n",
    "\n",
    "#### **Configuration Setup**\n",
    "\n",
    "Define essential BigQuery configuration directly in the notebook for\n",
    "complete self-containment:"
   ],
   "id": "bde35427-884d-4c78-9c97-e3124583c78b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# BigQuery Configuration - Legal Document Intelligence Platform\n",
    "# BigQuery AI Hackathon Configuration\n",
    "\n",
    "config = {\n",
    "    # Project Configuration\n",
    "    'project': {\n",
    "        'id': 'faizal-hackathon',\n",
    "        'location': 'US'\n",
    "    },\n",
    "\n",
    "\n",
    "    # Dataset Names (used in SQL queries)\n",
    "    'datasets': {\n",
    "        'legal_ai_platform': {\n",
    "            'subdatasets': {\n",
    "                'raw_data': 'legal_ai_platform_raw_data',\n",
    "                'vector_indexes': 'legal_ai_platform_vector_indexes'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # AI Model Names (used in ML function calls)\n",
    "    'ai_models': {\n",
    "        'ai_gemini_pro': 'ai_gemini_pro',\n",
    "        'text_embedding': 'text_embedding',\n",
    "        'timesfm': 'legal_timesfm'\n",
    "    },\n",
    "\n",
    "    # AI Connection Configuration (for AI.* functions)\n",
    "    'ai_connection': {\n",
    "        'connection_id': 'us.vertex_ai_connection',\n",
    "        'endpoint': 'gemini-2.0-flash'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… Configuration loaded successfully\")\n",
    "print(f\"Project ID: {config['project']['id']}\")\n",
    "print(f\"Available Datasets: {list(config['datasets']['legal_ai_platform']['subdatasets'].keys())}\")\n",
    "print(f\"Available AI Models: {list(config['ai_models'].keys())}\")\n",
    "print(f\"AI Connection: {config['ai_connection']['connection_id']}\")\n",
    "print(f\"AI Endpoint: {config['ai_connection']['endpoint']}\")"
   ],
   "id": "cc953c87-9664-4d0e-b70d-e0b14ce90304"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Google Cloud Authentication & AI Connection Setup**\n",
    "\n",
    "Set up authentication and configure the Vertex AI connection required\n",
    "for AI functions:"
   ],
   "id": "4f3c8186-f37c-4bbb-a621-2c56ed7b68c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up authentication\n",
    "# Option 1: Use service account key file (if available)\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'config/service-account-key.json'\n",
    "\n",
    "# Option 2: Use default authentication (recommended for Kaggle/Colab)\n",
    "# This will use the default service account or user credentials\n",
    "\n",
    "# Verify authentication and initialize BigQuery client\n",
    "from google.cloud import bigquery\n",
    "\n",
    "try:\n",
    "    client = bigquery.Client(project=config['project']['id'])\n",
    "    print(f\"âœ… Authenticated with project: {client.project}\")\n",
    "    print(f\"âœ… BigQuery client initialized successfully\")\n",
    "\n",
    "    # Test basic connectivity\n",
    "    test_query = \"SELECT 1 as test_connection\"\n",
    "    result = client.query(test_query).result()\n",
    "    test_value = next(result).test_connection\n",
    "    print(f\"âœ… Connection test successful (value: {test_value})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Authentication failed: {e}\")\n",
    "    print(\"ğŸ’¡ Please ensure you have proper Google Cloud credentials configured\")\n",
    "    print(\"   - For Kaggle: Use 'Add-ons' â†’ 'Google Cloud Services' â†’ 'BigQuery'\")\n",
    "    print(\"   - For local: Set GOOGLE_APPLICATION_CREDENTIALS environment variable\")\n",
    "    print(\"   - For Colab: Use 'Runtime' â†’ 'Change runtime type' â†’ 'Hardware accelerator' â†’ 'GPU' (optional)\")\n",
    "    print(\"\\nğŸ”— AI Functions Setup:\")\n",
    "    print(\"   - AI.GENERATE_TABLE and AI.GENERATE_BOOL use BigQuery AI models\")\n",
    "    print(\"   - AI.GENERATE_BOOL requires a BigQuery AI connection\")\n",
    "    print(\"   - Connection available: us.vertex_ai_connection\")\n",
    "    print(\"   - Grant Vertex AI User role to the connection's service account\")"
   ],
   "id": "cba3d294-2da9-4a41-ac65-f20f20cf3ca7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 BigQuery AI Connection Setup**\n",
    "\n",
    "**Note**: `AI.GENERATE_BOOL` requires a BigQuery AI connection. For\n",
    "competition environments, this connection may already be set up. If not,\n",
    "you can create it using the BigQuery console or the following steps:\n",
    "\n",
    "1.  **Create Connection** (if needed):\n",
    "\n",
    "    ``` bash\n",
    "    # Connection already exists: us.vertex_ai_connection\n",
    "    ```\n",
    "\n",
    "2.  **Grant Permissions** (if needed):\n",
    "\n",
    "    ``` bash\n",
    "    bq show --connection --location=US us.vertex_ai_connection\n",
    "    # Grant Vertex AI User role to the service account shown in the output\n",
    "    ```\n",
    "\n",
    "3.  **Verify Connection**:\n",
    "\n",
    "    ``` bash\n",
    "    bq query --use_legacy_sql=false \"SELECT 1 as test_connection\"\n",
    "    ```"
   ],
   "id": "c62805ba-49bb-424b-98aa-2281de60c1ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify BigQuery AI connection is available\n",
    "try:\n",
    "    # Test if AI.GENERATE_BOOL connection exists by running a simple test\n",
    "    test_query = \"\"\"\n",
    "    SELECT AI.GENERATE_BOOL('Test prompt', connection_id => 'us.vertex_ai_connection').result as test_result\n",
    "    \"\"\"\n",
    "\n",
    "    # Note: This will fail if connection doesn't exist, but that's expected\n",
    "    # The actual functions will handle this gracefully\n",
    "    print(\"âœ… BigQuery AI connection test prepared\")\n",
    "    print(\"ğŸ’¡ Connection will be tested when running AI.GENERATE_BOOL functions\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸ Connection test note: {e}\")\n",
    "    print(\"ğŸ’¡ This is expected if connection hasn't been set up yet\")"
   ],
   "id": "e744e630-350c-4f67-9884-91bb62cd6828"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Library Imports & Basic Setup**\n",
    "\n",
    "Import essential libraries and configure BigQuery connection:"
   ],
   "id": "f3c25b9e-be03-488c-96e5-af8cce3d99d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core BigQuery and AI libraries\n",
    "import bigframes\n",
    "import bigframes.pandas as bf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import GoogleCloudError\n",
    "\n",
    "# Data processing and utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Additional utilities\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure BigFrames\n",
    "bf.options.bigquery.project = config['project']['id']\n",
    "bf.options.bigquery.location = config['project']['location']\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")\n",
    "print(f\"âœ… BigFrames configured for project: {bf.options.bigquery.project}\")"
   ],
   "id": "b43bf773-2606-4d3b-a41b-cf22e8db9057"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Connection Verification**\n",
    "\n",
    "Verify BigQuery connection and check basic setup:"
   ],
   "id": "3e7ca07f-a445-4059-96a7-b29808d6487f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify BigQuery connection\n",
    "try:\n",
    "    # Test basic query\n",
    "    test_query = \"SELECT 1 as test_value\"\n",
    "    result = client.query(test_query).result()\n",
    "    test_value = next(result).test_value\n",
    "    print(f\"âœ… BigQuery connection verified (test value: {test_value})\")\n",
    "\n",
    "    # Check document count\n",
    "    count_query = f\"\"\"\n",
    "    SELECT COUNT(*) as document_count\n",
    "    FROM `{config['project']['id']}.legal_ai_platform_raw_data.legal_documents`\n",
    "    \"\"\"\n",
    "    result = client.query(count_query).result()\n",
    "    doc_count = next(result).document_count\n",
    "    print(f\"âœ… Legal documents available: {doc_count:,} documents\")\n",
    "\n",
    "    print(\"\\nğŸ‰ Setup complete! Ready to demonstrate BigQuery AI capabilities.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Setup verification failed: {e}\")\n",
    "    raise"
   ],
   "id": "94e26163-adc1-4fdc-9959-de1295f290ef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ready to transform legal document processing with BigQuery AI? Letâ€™s\n",
    "dive into the technical implementation!** ğŸš€"
   ],
   "id": "c96a9f55-8e54-4ed8-8f02-abffcac11b08"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š **Section 3: Data Acquisition & Loading**\n",
    "\n",
    "### **3.1 Legal Dataset Overview**\n",
    "\n",
    "Our Legal Document Intelligence Platform leverages high-quality legal\n",
    "datasets from Hugging Face, processed and stored in BigQuery for optimal\n",
    "AI processing performance."
   ],
   "id": "b2c5f4d7-d2ac-41f1-8129-88c11c1cb9dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore legal document dataset from Hugging Face\n",
    "def explore_legal_dataset():\n",
    "    \"\"\"Explore the legal document dataset and show key statistics.\"\"\"\n",
    "\n",
    "    print(\"ğŸ” Legal Dataset Exploration\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Check dataset overview\n",
    "    overview_query = f\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_documents,\n",
    "        COUNT(DISTINCT document_type) as document_types,\n",
    "        MIN(JSON_EXTRACT_SCALAR(metadata, '$.timestamp')) as earliest_case_date,\n",
    "        MAX(JSON_EXTRACT_SCALAR(metadata, '$.timestamp')) as latest_case_date,\n",
    "        AVG(LENGTH(content)) as avg_content_length,\n",
    "        MIN(LENGTH(content)) as min_content_length,\n",
    "        MAX(LENGTH(content)) as max_content_length\n",
    "    FROM `{config['project']['id']}.legal_ai_platform_raw_data.legal_documents`\n",
    "    WHERE content IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = client.query(overview_query).result()\n",
    "        overview = next(result)\n",
    "\n",
    "        print(f\"ğŸ“ˆ Dataset Statistics:\")\n",
    "        print(f\"  â€¢ Total Documents: {overview.total_documents:,}\")\n",
    "        print(f\"  â€¢ Document Types: {overview.document_types}\")\n",
    "        print(f\"  â€¢ Case Date Range: {overview.earliest_case_date} to {overview.latest_case_date}\")\n",
    "        print(f\"  â€¢ Average Content Length: {overview.avg_content_length:.0f} characters\")\n",
    "        print(f\"  â€¢ Content Range: {overview.min_content_length} - {overview.max_content_length} characters\")\n",
    "\n",
    "        return overview\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Dataset exploration failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run dataset exploration\n",
    "dataset_overview = explore_legal_dataset()"
   ],
   "id": "ab565922-100c-4791-b497-47d507a82627"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document types and distribution\n",
    "def analyze_document_types():\n",
    "    \"\"\"Analyze document type distribution and characteristics.\"\"\"\n",
    "\n",
    "    print(\"\\nğŸ“‹ Document Type Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Document type distribution\n",
    "    type_query = f\"\"\"\n",
    "    SELECT\n",
    "        document_type,\n",
    "        COUNT(*) as document_count,\n",
    "        AVG(LENGTH(content)) as avg_length,\n",
    "        MIN(LENGTH(content)) as min_length,\n",
    "        MAX(LENGTH(content)) as max_length\n",
    "    FROM `{config['project']['id']}.legal_ai_platform_raw_data.legal_documents`\n",
    "    WHERE content IS NOT NULL\n",
    "    GROUP BY document_type\n",
    "    ORDER BY document_count DESC\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = client.query(type_query).result()\n",
    "        doc_types = list(result)\n",
    "\n",
    "        print(f\"Document Type Distribution:\")\n",
    "        for doc_type in doc_types:\n",
    "            print(f\"  â€¢ {doc_type.document_type}: {doc_type.document_count:,} documents\")\n",
    "            print(f\"    - Avg Length: {doc_type.avg_length:.0f} characters\")\n",
    "            print(f\"    - Length Range: {doc_type.min_length} - {doc_type.max_length}\")\n",
    "\n",
    "        return doc_types\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Document type analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run document type analysis\n",
    "document_types = analyze_document_types()"
   ],
   "id": "cb60f220-dc34-4fac-9a32-c8f348a28f27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Data Validation & Quality Check**\n",
    "\n",
    "Letâ€™s validate the data quality and ensure itâ€™s ready for BigQuery AI\n",
    "processing:"
   ],
   "id": "ee61900a-afd6-48ab-8372-35d2f8877951"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality validation\n",
    "def validate_data_quality():\n",
    "    \"\"\"Validate data quality and completeness.\"\"\"\n",
    "\n",
    "    print(\"\\nâœ… Data Quality Validation\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Data completeness check\n",
    "    completeness_query = f\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(document_id) as non_null_ids,\n",
    "        COUNT(document_type) as non_null_types,\n",
    "        COUNT(content) as non_null_content,\n",
    "        COUNT(metadata) as non_null_metadata,\n",
    "        COUNT(created_at) as non_null_timestamps\n",
    "    FROM `{config['project']['id']}.legal_ai_platform_raw_data.legal_documents`\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = client.query(completeness_query).result()\n",
    "        completeness = next(result)\n",
    "\n",
    "        print(f\"ğŸ“Š Data Completeness:\")\n",
    "        print(f\"  â€¢ Total Rows: {completeness.total_rows:,}\")\n",
    "        print(f\"  â€¢ Document IDs: {completeness.non_null_ids:,} ({completeness.non_null_ids/completeness.total_rows*100:.1f}%)\")\n",
    "        print(f\"  â€¢ Document Types: {completeness.non_null_types:,} ({completeness.non_null_types/completeness.total_rows*100:.1f}%)\")\n",
    "        print(f\"  â€¢ Content: {completeness.non_null_content:,} ({completeness.non_null_content/completeness.total_rows*100:.1f}%)\")\n",
    "        print(f\"  â€¢ Metadata: {completeness.non_null_metadata:,} ({completeness.non_null_metadata/completeness.total_rows*100:.1f}%)\")\n",
    "        print(f\"  â€¢ Timestamps: {completeness.non_null_timestamps:,} ({completeness.non_null_timestamps/completeness.total_rows*100:.1f}%)\")\n",
    "\n",
    "        # Content quality check\n",
    "        content_quality_query = f\"\"\"\n",
    "        SELECT\n",
    "            COUNT(*) as total_docs,\n",
    "            COUNT(CASE WHEN LENGTH(content) > 100 THEN 1 END) as substantial_content,\n",
    "            COUNT(CASE WHEN LENGTH(content) > 1000 THEN 1 END) as detailed_content,\n",
    "            COUNT(CASE WHEN LENGTH(content) > 5000 THEN 1 END) as comprehensive_content\n",
    "        FROM `{config['project']['id']}.legal_ai_platform_raw_data.legal_documents`\n",
    "        WHERE content IS NOT NULL\n",
    "        \"\"\"\n",
    "\n",
    "        result = client.query(content_quality_query).result()\n",
    "        content_quality = next(result)\n",
    "\n",
    "        print(f\"\\nğŸ“ Content Quality:\")\n",
    "        print(f\"  â€¢ Substantial Content (>100 chars): {content_quality.substantial_content:,} ({content_quality.substantial_content/content_quality.total_docs*100:.1f}%)\")\n",
    "        print(f\"  â€¢ Detailed Content (>1000 chars): {content_quality.detailed_content:,} ({content_quality.detailed_content/content_quality.total_docs*100:.1f}%)\")\n",
    "        print(f\"  â€¢ Comprehensive Content (>5000 chars): {content_quality.comprehensive_content:,} ({content_quality.comprehensive_content/content_quality.total_docs*100:.1f}%)\")\n",
    "\n",
    "        return {\n",
    "            'completeness': completeness,\n",
    "            'content_quality': content_quality\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Data quality validation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run data quality validation\n",
    "quality_results = validate_data_quality()"
   ],
   "id": "16788011-933c-4809-9b6e-1de2a64f5071"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data readiness summary\n",
    "def data_readiness_summary():\n",
    "    \"\"\"Provide summary of data readiness for AI processing.\"\"\"\n",
    "\n",
    "    print(\"\\nğŸš€ Data Readiness Summary\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if dataset_overview and quality_results:\n",
    "        print(\"âœ… Data Status: READY FOR AI PROCESSING\")\n",
    "        print(f\"\\nğŸ“Š Key Metrics:\")\n",
    "        print(f\"  â€¢ Total Documents Available: {dataset_overview.total_documents:,}\")\n",
    "        print(f\"  â€¢ Data Completeness: {quality_results['completeness'].non_null_content/quality_results['completeness'].total_rows*100:.1f}%\")\n",
    "        print(f\"  â€¢ Average Document Length: {dataset_overview.avg_content_length:.0f} characters\")\n",
    "\n",
    "        print(f\"\\nğŸ¯ Ready for BigQuery AI Functions:\")\n",
    "        print(f\"  â€¢ ML.GENERATE_TEXT: âœ… Document summarization\")\n",
    "        print(f\"  â€¢ AI.GENERATE_TABLE: âœ… Data extraction\")\n",
    "        print(f\"  â€¢ AI.GENERATE_BOOL: âœ… Urgency detection\")\n",
    "        print(f\"  â€¢ ML.GENERATE_EMBEDDING: âœ… Vector embeddings\")\n",
    "        print(f\"  â€¢ VECTOR_SEARCH: âœ… Similarity search\")\n",
    "\n",
    "        print(f\"\\nğŸ’¼ Business Impact Potential:\")\n",
    "        print(f\"  â€¢ Documents ready for processing: {dataset_overview.total_documents:,}\")\n",
    "        print(f\"  â€¢ Estimated time savings: {dataset_overview.total_documents * 15} minutes (manual processing)\")\n",
    "        print(f\"  â€¢ AI processing potential: {dataset_overview.total_documents * 2.17} seconds (estimated)\")\n",
    "\n",
    "    else:\n",
    "        print(\"âŒ Data Status: NOT READY - Please check data loading and validation\")\n",
    "\n",
    "    print(f\"\\nğŸ‰ Data preparation complete! Ready to demonstrate BigQuery AI capabilities.\")\n",
    "\n",
    "# Run data readiness summary\n",
    "data_readiness_summary()"
   ],
   "id": "7a83359d-bf53-4f1a-8fe6-760c87ec0e65"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  **Section 4: Track 1 - Generative AI Functions Implementation**\n",
    "\n",
    "### **4.1 ML.GENERATE_TEXT - Document Summarization**\n",
    "\n",
    "Letâ€™s implement the ML.GENERATE_TEXT function to automatically summarize\n",
    "legal documents using BigQuery AI. This demonstrates how we can extract\n",
    "key insights from lengthy legal documents in seconds."
   ],
   "id": "dc1ecb6e-a287-4da6-aa44-f60c127c7d99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_generate_text(document_id=None, limit=10):\n",
    "    \"\"\"\n",
    "    Implement ML.GENERATE_TEXT for document summarization using BigQuery AI.\n",
    "\n",
    "    Args:\n",
    "        document_id: Specific document ID to summarize (optional)\n",
    "        limit: Number of documents to process (default: 10)\n",
    "\n",
    "    Returns:\n",
    "        Dict containing summarization results\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸš€ Starting ML.GENERATE_TEXT summarization...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Connect to BigQuery\n",
    "        if not client:\n",
    "            raise Exception(\"BigQuery client not initialized\")\n",
    "\n",
    "        # Build parameterized query to prevent SQL injection\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "            document_id,\n",
    "            document_type,\n",
    "            ml_generate_text_llm_result AS summary,\n",
    "            ml_generate_text_status AS status\n",
    "        FROM ML.GENERATE_TEXT(\n",
    "            MODEL `{project_id}.ai_models.ai_gemini_pro`,\n",
    "            (\n",
    "                SELECT\n",
    "                    document_id,\n",
    "                    document_type,\n",
    "                    CONCAT(\n",
    "                        'Summarize this legal document. Focus on key legal issues, outcomes, and important details. Start directly with the summary without introductory phrases: ',\n",
    "                        content\n",
    "                    ) AS prompt\n",
    "                FROM `{project_id}.legal_ai_platform_raw_data.legal_documents`\n",
    "                {where_clause}\n",
    "            ),\n",
    "            STRUCT(\n",
    "                TRUE AS flatten_json_output,\n",
    "                2048 AS max_output_tokens,\n",
    "                0.1 AS temperature,\n",
    "                0.8 AS top_p,\n",
    "                40 AS top_k\n",
    "            )\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # Build where clause based on parameters\n",
    "        where_clause = \"\"\n",
    "        if document_id:\n",
    "            where_clause = f\"WHERE document_id = '{document_id}'\"\n",
    "        else:\n",
    "            where_clause = f\"ORDER BY created_at DESC LIMIT {limit}\"\n",
    "\n",
    "        # Format query with project ID and where clause\n",
    "        query = query.format(\n",
    "            project_id=config['project']['id'],\n",
    "            where_clause=where_clause\n",
    "        )\n",
    "\n",
    "        print(\"ğŸ“ Executing ML.GENERATE_TEXT query...\")\n",
    "        result = client.query(query)\n",
    "\n",
    "        # Process results\n",
    "        summaries = []\n",
    "        for row in result:\n",
    "            if row.status:\n",
    "                print(f\"âš ï¸  Document {row.document_id} has status: {row.status}\")\n",
    "\n",
    "            # Debug: Check what we're getting from BigQuery\n",
    "            print(f\"ğŸ” Document {row.document_id}:\")\n",
    "            print(f\"  Summary length: {len(str(row.summary)) if row.summary else 0} characters\")\n",
    "            print(f\"  Summary preview: {str(row.summary)[:100] if row.summary else 'None'}...\")\n",
    "\n",
    "            summary_data = {\n",
    "                'document_id': row.document_id,\n",
    "                'document_type': row.document_type,\n",
    "                'summary': row.summary or \"No summary generated\",\n",
    "                'status': row.status or \"OK\",\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            }\n",
    "            summaries.append(summary_data)\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        print(f\"âœ… Generated {len(summaries)} document summaries using ML.GENERATE_TEXT\")\n",
    "        print(f\"â±ï¸  Processing time: {processing_time:.2f} seconds\")\n",
    "        print(f\"ğŸ“Š Average time per document: {processing_time/len(summaries):.2f} seconds\")\n",
    "\n",
    "        return {\n",
    "            'function': 'ML.GENERATE_TEXT',\n",
    "            'purpose': 'Document Summarization',\n",
    "            'total_documents': len(summaries),\n",
    "            'summaries': summaries,\n",
    "            'processing_time': processing_time,\n",
    "            'avg_time_per_doc': processing_time/len(summaries),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ML.GENERATE_TEXT summarization failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test the function and store results for analysis\n",
    "print(\"ğŸ§ª Testing ML.GENERATE_TEXT function...\")\n",
    "try:\n",
    "    # Run ML.GENERATE_TEXT and store results\n",
    "    ml_generate_text_result = ml_generate_text(limit=3)\n",
    "    print(f\"âœ… Function test successful!\")\n",
    "    print(f\"ğŸ“ˆ Processed {ml_generate_text_result['total_documents']} documents\")\n",
    "    print(f\"âš¡ Average processing time: {ml_generate_text_result['avg_time_per_doc']:.2f}s per document\")\n",
    "\n",
    "    # Store result for analysis functions\n",
    "    result = ml_generate_text_result\n",
    "    print(f\"ğŸ’¾ Results stored in 'result' variable for analysis\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Function test failed: {e}\")\n",
    "    print(f\"ğŸ’¡ Make sure BigQuery client is connected and data is available\")"
   ],
   "id": "c6c6d0dd-0464-4d3b-b9f7-ef5cb8f2c72d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ML.GENERATE_TEXT Results Analysis**\n",
    "\n",
    "Letâ€™s analyze the results and demonstrate the business impact of\n",
    "automated document summarization:"
   ],
   "id": "c65d6fbb-620a-4eed-bc7b-8e7921787ffa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ML.GENERATE_TEXT results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_summarization_results(result):\n",
    "    \"\"\"Analyze and visualize ML.GENERATE_TEXT results.\"\"\"\n",
    "\n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(result['summaries'])\n",
    "\n",
    "    print(\"ğŸ“Š ML.GENERATE_TEXT Results Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Basic statistics\n",
    "    print(f\"Total Documents Processed: {len(df)}\")\n",
    "    print(f\"Processing Time: {result['processing_time']:.2f} seconds\")\n",
    "    print(f\"Average Time per Document: {result['avg_time_per_doc']:.2f} seconds\")\n",
    "\n",
    "    # Document type distribution\n",
    "    print(f\"\\nğŸ“‹ Document Type Distribution:\")\n",
    "    doc_types = df['document_type'].value_counts()\n",
    "    for doc_type, count in doc_types.items():\n",
    "        print(f\"  {doc_type}: {count} documents\")\n",
    "\n",
    "    # Status analysis\n",
    "    print(f\"\\nâœ… Status Analysis:\")\n",
    "    status_counts = df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} documents\")\n",
    "\n",
    "    # Show sample summaries as markdown table for judges\n",
    "    print(f\"\\nğŸ“ Sample Summaries:\")\n",
    "    print(f\"\\n## ML.GENERATE_TEXT Results - Legal Document Summarization\")\n",
    "    print(f\"\\n| Document ID | Type | Summary Preview | Length | Status |\")\n",
    "    print(f\"|-------------|------|-----------------|--------|--------|\")\n",
    "\n",
    "    for i, row in df.head(5).iterrows():\n",
    "        summary_preview = str(row['summary'])[:100] + '...' if len(str(row['summary'])) > 100 else str(row['summary'])\n",
    "        summary_length = len(str(row['summary']))\n",
    "\n",
    "        print(f\"| {row['document_id']} | {row['document_type']} | {summary_preview} | {summary_length} chars | {row['status']} |\")\n",
    "\n",
    "    print(f\"\\n**Summarization Summary:**\")\n",
    "    print(f\"- Total Documents: {len(df)}\")\n",
    "    print(f\"- Processing Time: {result['processing_time']:.2f} seconds\")\n",
    "    print(f\"- Average Time per Document: {result['avg_time_per_doc']:.2f} seconds\")\n",
    "    print(f\"- Success Rate: {len(df[df['status'] == 'OK'])}/{len(df)} documents\")\n",
    "\n",
    "    # Calculate business impact\n",
    "    print(f\"\\nğŸ’¼ Business Impact Analysis:\")\n",
    "    print(f\"Time Saved per Document: ~15 minutes (manual) vs {result['avg_time_per_doc']:.2f}s (AI)\")\n",
    "    time_saved_per_doc = 15 * 60 - result['avg_time_per_doc']  # 15 minutes in seconds\n",
    "    total_time_saved = time_saved_per_doc * len(df)\n",
    "    print(f\"Total Time Saved: {total_time_saved/60:.1f} minutes for {len(df)} documents\")\n",
    "    print(f\"Efficiency Improvement: {(time_saved_per_doc / (15*60)) * 100:.1f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run analysis\n",
    "if 'result' in locals() and isinstance(result, dict) and 'summaries' in result:\n",
    "    df_results = analyze_summarization_results(result)\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for analysis. Please run ml_generate_text() first.\")\n",
    "    print(\"ğŸ’¡ Tip: Make sure to run the ml_generate_text() function to get results for analysis.\")"
   ],
   "id": "d099d452-4704-4a5f-9b1f-f99a9ca13d34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ML.GENERATE_TEXT Quality Assessment**\n",
    "\n",
    "Letâ€™s also show the original document content alongside the AI-generated\n",
    "summaries for quality evaluation:"
   ],
   "id": "685e76cd-c3e2-49e7-a4c5-5c45b86da9d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original content vs AI summary for quality assessment\n",
    "def show_content_vs_summary(result):\n",
    "    \"\"\"Show original document content alongside AI-generated summaries.\"\"\"\n",
    "\n",
    "    if not result or 'summaries' not in result:\n",
    "        print(\"âš ï¸  No results available for content comparison\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸ” Content vs Summary Quality Assessment\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Get original content for comparison\n",
    "    for i, summary_data in enumerate(result['summaries'][:2], 1):  # Show first 2 for detailed review\n",
    "        doc_id = summary_data['document_id']\n",
    "\n",
    "        # Get original content\n",
    "        content_query = f\"\"\"\n",
    "        SELECT content, document_type, metadata\n",
    "        FROM `{config['project']['id']}.legal_ai_platform_raw_data.legal_documents`\n",
    "        WHERE document_id = '{doc_id}'\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            content_result = client.query(content_query).result()\n",
    "            original_doc = next(content_result)\n",
    "\n",
    "            print(f\"\\n{'='*100}\")\n",
    "            print(f\"DOCUMENT {i}: {doc_id} ({summary_data['document_type']})\")\n",
    "            print(f\"{'='*100}\")\n",
    "\n",
    "            print(f\"\\nğŸ“„ ORIGINAL CONTENT (First 500 characters):\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            print(f\"{original_doc.content[:500]}...\")\n",
    "            print(f\"\\n[Total Length: {len(original_doc.content):,} characters]\")\n",
    "\n",
    "            print(f\"\\nğŸ¤– AI-GENERATED SUMMARY:\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            print(f\"{summary_data['summary']}\")\n",
    "\n",
    "            print(f\"\\nğŸ“Š SUMMARY ANALYSIS:\")\n",
    "            print(f\"  â€¢ Original Length: {len(original_doc.content):,} characters\")\n",
    "            print(f\"  â€¢ Summary Length: {len(summary_data['summary']):,} characters\")\n",
    "            print(f\"  â€¢ Compression Ratio: {len(original_doc.content)/len(summary_data['summary']):.1f}:1\")\n",
    "            print(f\"  â€¢ Processing Status: {summary_data['status']}\")\n",
    "\n",
    "            if original_doc.metadata:\n",
    "                print(f\"\\nğŸ“‹ METADATA:\")\n",
    "                print(f\"  {original_doc.metadata}\")\n",
    "\n",
    "            print(f\"{'='*100}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to get original content for {doc_id}: {e}\")\n",
    "\n",
    "    print(f\"\\nâœ… Quality Assessment Complete\")\n",
    "\n",
    "# Run content vs summary comparison\n",
    "if 'result' in locals() and isinstance(result, dict) and 'summaries' in result:\n",
    "    show_content_vs_summary(result)\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for content comparison. Please run ml_generate_text() first.\")"
   ],
   "id": "aea8a881-e332-4028-b6dd-3ab2f39f4b55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 AI.GENERATE_TABLE - Data Extraction**\n",
    "\n",
    "Letâ€™s implement the AI.GENERATE_TABLE function to extract structured\n",
    "legal data from documents. This demonstrates how we can automatically\n",
    "extract key legal entities and information in a structured format."
   ],
   "id": "ffd444ff-87ff-4f60-b684-b35a92840da5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_generate_table(document_id=None, limit=10):\n",
    "    \"\"\"\n",
    "    Implement AI.GENERATE_TABLE for structured data extraction using BigQuery AI.\n",
    "\n",
    "    Args:\n",
    "        document_id: Specific document ID to extract from (optional)\n",
    "        limit: Number of documents to process (default: 10)\n",
    "\n",
    "    Returns:\n",
    "        Dict containing extraction results\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸš€ Starting AI.GENERATE_TABLE data extraction...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Connect to BigQuery\n",
    "        if not client:\n",
    "            raise Exception(\"BigQuery client not initialized\")\n",
    "\n",
    "        # Build parameterized query for structured data extraction using AI.GENERATE_TABLE\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "            document_id,\n",
    "            document_type,\n",
    "            case_number,\n",
    "            court_name,\n",
    "            plaintiff,\n",
    "            defendant,\n",
    "            outcome,\n",
    "            status\n",
    "            FROM AI.GENERATE_TABLE(\n",
    "            MODEL `{project_id}.ai_models.ai_gemini_pro`,\n",
    "            (\n",
    "                SELECT\n",
    "                    document_id,\n",
    "                    document_type,\n",
    "                    CONCAT(\n",
    "                        'Extract available legal information as structured data. Return a JSON object with these fields if available: case_number, court_name, case_date, plaintiff, defendant, monetary_amount, legal_issues, outcome. If a field is not available in the document, omit it from the result.',\n",
    "                        content\n",
    "                    ) AS prompt\n",
    "                FROM `{project_id}.legal_ai_platform_raw_data.legal_documents`\n",
    "                {where_clause}\n",
    "            ),\n",
    "            STRUCT(\n",
    "                \"case_number STRING, court_name STRING, plaintiff STRING, defendant STRING, outcome STRING\" AS output_schema,\n",
    "              1024 AS max_output_tokens\n",
    "            )\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        if document_id:\n",
    "            where_clause = f\"WHERE document_id = '{document_id}'\"\n",
    "        else:\n",
    "            where_clause = f\"ORDER BY created_at DESC LIMIT {limit}\"\n",
    "\n",
    "        # Format query with project ID and where clause\n",
    "        query = query.format(\n",
    "            project_id=config['project']['id'],\n",
    "            where_clause=where_clause\n",
    "        )\n",
    "\n",
    "        print(\"ğŸ“ Executing AI.GENERATE_TABLE query...\")\n",
    "        result = client.query(query)\n",
    "\n",
    "        # Process results\n",
    "        extractions = []\n",
    "        for row in result:\n",
    "            if row.status:\n",
    "                print(f\"âš ï¸  Document {row.document_id} has status: {row.status}\")\n",
    "\n",
    "            # Debug: Check what we're getting from BigQuery\n",
    "            print(f\"ğŸ” Document {row.document_id}:\")\n",
    "            print(f\"  Case Number: {row.case_number}\")\n",
    "            print(f\"  Court Name: {row.court_name}\")\n",
    "            print(f\"  Plaintiff: {row.plaintiff}\")\n",
    "            print(f\"  Defendant: {row.defendant}\")\n",
    "\n",
    "            # Create structured extraction data from direct schema columns\n",
    "            extracted_data = {\n",
    "                'case_number': row.case_number,\n",
    "                'court_name': row.court_name,\n",
    "                'plaintiff': row.plaintiff,\n",
    "                'defendant': row.defendant,\n",
    "                'outcome': row.outcome\n",
    "            }\n",
    "\n",
    "            extraction_data = {\n",
    "                'document_id': row.document_id,\n",
    "                'document_type': row.document_type,\n",
    "                'extracted_data': extracted_data,\n",
    "                'status': row.status or \"OK\",\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            }\n",
    "            extractions.append(extraction_data)\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        print(f\"âœ… Generated {len(extractions)} data extractions using AI.GENERATE_TABLE\")\n",
    "        print(f\"â±ï¸  Processing time: {processing_time:.2f} seconds\")\n",
    "        print(f\"ğŸ“Š Average time per document: {processing_time/len(extractions):.2f} seconds\")\n",
    "\n",
    "        return {\n",
    "            'function': 'AI.GENERATE_TABLE',\n",
    "            'purpose': 'Structured Legal Data Extraction',\n",
    "            'total_documents': len(extractions),\n",
    "            'extractions': extractions,\n",
    "            'processing_time': processing_time,\n",
    "            'avg_time_per_doc': processing_time/len(extractions),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ AI.GENERATE_TABLE extraction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test the function and store results for analysis\n",
    "print(\"ğŸ§ª Testing AI.GENERATE_TABLE function...\")\n",
    "try:\n",
    "    # Run AI.GENERATE_TABLE and store results\n",
    "    ai_generate_table_result = ai_generate_table(limit=3)\n",
    "    print(f\"âœ… Function test successful!\")\n",
    "    print(f\"ğŸ“ˆ Processed {ai_generate_table_result['total_documents']} documents\")\n",
    "    print(f\"âš¡ Average processing time: {ai_generate_table_result['avg_time_per_doc']:.2f}s per document\")\n",
    "\n",
    "    # Store result for analysis functions\n",
    "    table_result = ai_generate_table_result\n",
    "    print(f\"ğŸ’¾ Results stored in 'table_result' variable for analysis\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Function test failed: {e}\")\n",
    "    print(f\"ğŸ’¡ Make sure BigQuery client is connected and data is available\")"
   ],
   "id": "be660aa5-ee37-4fc9-a50b-06049331abcf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI.GENERATE_TABLE Results Analysis**\n",
    "\n",
    "Letâ€™s analyze the structured data extraction results and demonstrate the\n",
    "business impact:"
   ],
   "id": "c33e5c54-5f72-41b5-b9ca-93a009fd8d1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze AI.GENERATE_TABLE results\n",
    "def analyze_extraction_results(result):\n",
    "    \"\"\"Analyze and visualize AI.GENERATE_TABLE results.\"\"\"\n",
    "    import json\n",
    "\n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(result['extractions'])\n",
    "\n",
    "    print(\"ğŸ“Š AI.GENERATE_TABLE Results Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Basic statistics\n",
    "    print(f\"Total Documents Processed: {len(df)}\")\n",
    "    print(f\"Processing Time: {result['processing_time']:.2f} seconds\")\n",
    "    print(f\"Average Time per Document: {result['avg_time_per_doc']:.2f} seconds\")\n",
    "\n",
    "    # Document type distribution\n",
    "    print(f\"\\nğŸ“‹ Document Type Distribution:\")\n",
    "    doc_types = df['document_type'].value_counts()\n",
    "    for doc_type, count in doc_types.items():\n",
    "        print(f\"  {doc_type}: {count} documents\")\n",
    "\n",
    "    # Status analysis\n",
    "    print(f\"\\nâœ… Status Analysis:\")\n",
    "    status_counts = df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} documents\")\n",
    "\n",
    "    # Show sample extractions as markdown table\n",
    "    print(f\"\\nğŸ“ Sample Extractions:\")\n",
    "    print(f\"\\n## AI.GENERATE_TABLE Results - Structured Legal Data Extraction\")\n",
    "    print(f\"\\n| Document ID | Type | Case Number | Court | Plaintiff | Defendant | Amount | Issues | Outcome |\")\n",
    "    print(f\"|-------------|------|-------------|-------|-----------|-----------|--------|--------|---------|\")\n",
    "\n",
    "    for i, row in df.head(5).iterrows():\n",
    "        extracted = row['extracted_data']\n",
    "        case_num = extracted.get('case_number', 'N/A')[:20] + '...' if len(str(extracted.get('case_number', ''))) > 20 else extracted.get('case_number', 'N/A')\n",
    "        court = extracted.get('court_name', 'N/A')[:15] + '...' if len(str(extracted.get('court_name', ''))) > 15 else extracted.get('court_name', 'N/A')\n",
    "        plaintiff = extracted.get('plaintiff', 'N/A')[:15] + '...' if len(str(extracted.get('plaintiff', ''))) > 15 else extracted.get('plaintiff', 'N/A')\n",
    "        defendant = extracted.get('defendant', 'N/A')[:15] + '...' if len(str(extracted.get('defendant', ''))) > 15 else extracted.get('defendant', 'N/A')\n",
    "        amount = extracted.get('monetary_amount', 'N/A')\n",
    "        issues = extracted.get('legal_issues', 'N/A')[:20] + '...' if len(str(extracted.get('legal_issues', ''))) > 20 else extracted.get('legal_issues', 'N/A')\n",
    "        outcome = extracted.get('outcome', 'N/A')[:15] + '...' if len(str(extracted.get('outcome', ''))) > 15 else extracted.get('outcome', 'N/A')\n",
    "\n",
    "        print(f\"| {row['document_id']} | {row['document_type']} | {case_num} | {court} | {plaintiff} | {defendant} | {amount} | {issues} | {outcome} |\")\n",
    "\n",
    "    print(f\"\\n**Processing Summary:**\")\n",
    "    print(f\"- Total Documents: {len(df)}\")\n",
    "    print(f\"- Processing Time: {result['processing_time']:.2f} seconds\")\n",
    "    print(f\"- Average Time per Document: {result['avg_time_per_doc']:.2f} seconds\")\n",
    "    print(f\"- Success Rate: {len(df[df['status'] == 'OK'])}/{len(df)} documents\")\n",
    "\n",
    "    # Calculate business impact\n",
    "    print(f\"\\nğŸ’¼ Business Impact Analysis:\")\n",
    "    print(f\"Time Saved per Document: ~20 minutes (manual) vs {result['avg_time_per_doc']:.2f}s (AI)\")\n",
    "    time_saved_per_doc = 20 * 60 - result['avg_time_per_doc']  # 20 minutes in seconds\n",
    "    total_time_saved = time_saved_per_doc * len(df)\n",
    "    print(f\"Total Time Saved: {total_time_saved/60:.1f} minutes for {len(df)} documents\")\n",
    "    print(f\"Efficiency Improvement: {(time_saved_per_doc / (20*60)) * 100:.1f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run analysis\n",
    "if 'table_result' in locals() and isinstance(table_result, dict) and 'extractions' in table_result:\n",
    "    df_extractions = analyze_extraction_results(table_result)\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for analysis. Please run ai_generate_table() first.\")\n",
    "    print(\"ğŸ’¡ Tip: Make sure to run the ai_generate_table() function to get results for analysis.\")"
   ],
   "id": "d5455196-aa11-4db9-b849-cfc58ed35fbb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI.GENERATE_TABLE Quality Assessment**\n",
    "\n",
    "Letâ€™s show the original document content alongside the extracted\n",
    "structured data for quality evaluation:"
   ],
   "id": "d731cb15-bf11-41cd-af9f-f1a05f321a0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original content vs extracted data for quality assessment\n",
    "def show_content_vs_extraction(result):\n",
    "    \"\"\"Show original document content alongside extracted structured data.\"\"\"\n",
    "    import json\n",
    "\n",
    "    if not result or 'extractions' not in result:\n",
    "        print(\"âš ï¸  No results available for content comparison\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸ” Content vs Extraction Quality Assessment\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Get original content for comparison\n",
    "    for i, extraction_data in enumerate(result['extractions'][:2], 1):  # Show first 2 for detailed review\n",
    "        doc_id = extraction_data['document_id']\n",
    "\n",
    "        # Get original content\n",
    "        content_query = f\"\"\"\n",
    "        SELECT content, document_type, metadata\n",
    "        FROM `{config['project']['id']}.legal_ai_platform_raw_data.legal_documents`\n",
    "        WHERE document_id = '{doc_id}'\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            content_result = client.query(content_query).result()\n",
    "            original_doc = next(content_result)\n",
    "\n",
    "            print(f\"\\n{'='*100}\")\n",
    "            print(f\"DOCUMENT {i}: {doc_id} ({extraction_data['document_type']})\")\n",
    "            print(f\"{'='*100}\")\n",
    "\n",
    "            print(f\"\\nğŸ“„ ORIGINAL CONTENT (First 500 characters):\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            print(f\"{original_doc.content[:500]}...\")\n",
    "            print(f\"\\n[Total Length: {len(original_doc.content):,} characters]\")\n",
    "\n",
    "            print(f\"\\nğŸ¤– AI-EXTRACTED STRUCTURED DATA:\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            print(f\"{json.dumps(extraction_data['extracted_data'], indent=2)}\")\n",
    "\n",
    "            print(f\"\\nğŸ“Š EXTRACTION ANALYSIS:\")\n",
    "            print(f\"  â€¢ Original Length: {len(original_doc.content):,} characters\")\n",
    "            print(f\"  â€¢ Extracted Fields: {len(extraction_data['extracted_data'])} fields\")\n",
    "            print(f\"  â€¢ Processing Status: {extraction_data['status']}\")\n",
    "\n",
    "            # Show extracted fields (only available fields will be present)\n",
    "            if extraction_data['extracted_data']:\n",
    "                print(f\"\\nğŸ“‹ EXTRACTED FIELDS:\")\n",
    "                for field, value in extraction_data['extracted_data'].items():\n",
    "                    if field != 'error':\n",
    "                        print(f\"  â€¢ {field}: {value}\")\n",
    "\n",
    "            if original_doc.metadata:\n",
    "                print(f\"\\nğŸ“‹ METADATA:\")\n",
    "                print(f\"  {original_doc.metadata}\")\n",
    "\n",
    "            print(f\"{'='*100}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to get original content for {doc_id}: {e}\")\n",
    "\n",
    "    print(f\"\\nâœ… Quality Assessment Complete\")\n",
    "\n",
    "# Run content vs extraction comparison\n",
    "if 'table_result' in locals() and isinstance(table_result, dict) and 'extractions' in table_result:\n",
    "    show_content_vs_extraction(table_result)\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for content comparison. Please run ai_generate_table() first.\")"
   ],
   "id": "3a3d8cc8-b34a-4667-b33d-19187cc418e7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3 AI.GENERATE_BOOL - Urgency Detection**\n",
    "\n",
    "Letâ€™s implement the AI.GENERATE_BOOL function to classify document\n",
    "urgency using boolean output. This demonstrates how we can automatically\n",
    "detect time-sensitive legal matters that require immediate attention."
   ],
   "id": "04a476c9-5131-4b80-ba9a-abc83894a37f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_generate_bool(document_id=None, limit=10):\n",
    "    \"\"\"\n",
    "    Implement AI.GENERATE_BOOL for urgency detection using BigQuery AI.\n",
    "\n",
    "    Args:\n",
    "        document_id: Specific document ID to analyze (optional)\n",
    "        limit: Number of documents to process (default: 10)\n",
    "\n",
    "    Returns:\n",
    "        Dict containing urgency analysis results\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸš€ Starting AI.GENERATE_BOOL urgency detection...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Connect to BigQuery\n",
    "        if not client:\n",
    "            raise Exception(\"BigQuery client not initialized\")\n",
    "\n",
    "        # Build parameterized query for boolean classification using AI.GENERATE_BOOL\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "            document_id,\n",
    "            document_type,\n",
    "            AI.GENERATE_BOOL(\n",
    "                CONCAT(\n",
    "                    'Analyze this legal document for urgency. Consider factors like deadlines, time-sensitive matters, emergency situations, or immediate action required. Is this document urgent? ',\n",
    "                    content\n",
    "                ),\n",
    "                connection_id => 'us.vertex_ai_connection'\n",
    "            ).result AS is_urgent,\n",
    "            AI.GENERATE_BOOL(\n",
    "                CONCAT(\n",
    "                    'Analyze this legal document for urgency. Consider factors like deadlines, time-sensitive matters, emergency situations, or immediate action required. Is this document urgent? ',\n",
    "                    content\n",
    "                ),\n",
    "                connection_id => 'us.vertex_ai_connection'\n",
    "            ).status AS status\n",
    "        FROM `{project_id}.legal_ai_platform_raw_data.legal_documents`\n",
    "        {where_clause}\n",
    "        \"\"\"\n",
    "\n",
    "        # Build where clause based on parameters\n",
    "        where_clause = \"\"\n",
    "        if document_id:\n",
    "            where_clause = f\"WHERE document_id = '{document_id}'\"\n",
    "        else:\n",
    "            where_clause = f\"ORDER BY created_at DESC LIMIT {limit}\"\n",
    "\n",
    "        # Format query with project ID and where clause\n",
    "        query = query.format(\n",
    "            project_id=config['project']['id'],\n",
    "            where_clause=where_clause\n",
    "        )\n",
    "\n",
    "        print(\"ğŸ“ Executing AI.GENERATE_BOOL query...\")\n",
    "        result = client.query(query)\n",
    "\n",
    "        # Process results\n",
    "        urgency_analyses = []\n",
    "        for row in result:\n",
    "            if row.status:\n",
    "                print(f\"âš ï¸  Document {row.document_id} has status: {row.status}\")\n",
    "\n",
    "            # Debug: Check what we're getting from BigQuery\n",
    "            print(f\"ğŸ” Document {row.document_id}:\")\n",
    "            print(f\"  Urgency result: {row.is_urgent} (type: {type(row.is_urgent)})\")\n",
    "\n",
    "            # Handle boolean result (AI.GENERATE_BOOL returns actual boolean)\n",
    "            is_urgent = bool(row.is_urgent) if row.is_urgent is not None else False\n",
    "            urgency_text = \"URGENT\" if is_urgent else \"NOT_URGENT\"\n",
    "\n",
    "            urgency_data = {\n",
    "                'document_id': row.document_id,\n",
    "                'document_type': row.document_type,\n",
    "                'is_urgent': is_urgent,\n",
    "                'urgency_text': urgency_text,\n",
    "                'status': row.status or \"OK\",\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            }\n",
    "            urgency_analyses.append(urgency_data)\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        print(f\"âœ… Generated {len(urgency_analyses)} urgency analyses using AI.GENERATE_BOOL\")\n",
    "        print(f\"â±ï¸  Processing time: {processing_time:.2f} seconds\")\n",
    "        print(f\"ğŸ“Š Average time per document: {processing_time/len(urgency_analyses):.2f} seconds\")\n",
    "\n",
    "        return {\n",
    "            'function': 'AI.GENERATE_BOOL',\n",
    "            'purpose': 'Document Urgency Detection',\n",
    "            'total_documents': len(urgency_analyses),\n",
    "            'urgency_analyses': urgency_analyses,\n",
    "            'processing_time': processing_time,\n",
    "            'avg_time_per_doc': processing_time/len(urgency_analyses),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ AI.GENERATE_BOOL urgency detection failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test the function and store results for analysis\n",
    "print(\"ğŸ§ª Testing AI.GENERATE_BOOL function...\")\n",
    "try:\n",
    "    # Run AI.GENERATE_BOOL and store results\n",
    "    ai_generate_bool_result = ai_generate_bool(limit=3)\n",
    "    print(f\"âœ… Function test successful!\")\n",
    "    print(f\"ğŸ“ˆ Processed {ai_generate_bool_result['total_documents']} documents\")\n",
    "    print(f\"âš¡ Average processing time: {ai_generate_bool_result['avg_time_per_doc']:.2f}s per document\")\n",
    "\n",
    "    # Store result for analysis functions\n",
    "    bool_result = ai_generate_bool_result\n",
    "    print(f\"ğŸ’¾ Results stored in 'bool_result' variable for analysis\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Function test failed: {e}\")\n",
    "    print(f\"ğŸ’¡ Make sure BigQuery client is connected and data is available\")"
   ],
   "id": "948c2bfd-83ab-46a3-a134-c21b9f7a8548"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI.GENERATE_BOOL Results Analysis**\n",
    "\n",
    "Letâ€™s analyze the urgency detection results and demonstrate the business\n",
    "impact:"
   ],
   "id": "6ba642ce-c427-41fc-bf66-a62cec6c36ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze AI.GENERATE_BOOL results\n",
    "def analyze_urgency_results(result):\n",
    "    \"\"\"Analyze and visualize AI.GENERATE_BOOL results.\"\"\"\n",
    "\n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(result['urgency_analyses'])\n",
    "\n",
    "    print(\"ğŸ“Š AI.GENERATE_BOOL Results Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Basic statistics\n",
    "    print(f\"Total Documents Processed: {len(df)}\")\n",
    "    print(f\"Processing Time: {result['processing_time']:.2f} seconds\")\n",
    "    print(f\"Average Time per Document: {result['avg_time_per_doc']:.2f} seconds\")\n",
    "\n",
    "    # Document type distribution\n",
    "    print(f\"\\nğŸ“‹ Document Type Distribution:\")\n",
    "    doc_types = df['document_type'].value_counts()\n",
    "    for doc_type, count in doc_types.items():\n",
    "        print(f\"  {doc_type}: {count} documents\")\n",
    "\n",
    "    # Urgency analysis\n",
    "    print(f\"\\nğŸš¨ Urgency Analysis:\")\n",
    "    urgency_counts = df['is_urgent'].value_counts()\n",
    "    urgent_docs = urgency_counts.get(True, 0)\n",
    "    non_urgent_docs = urgency_counts.get(False, 0)\n",
    "    total_docs = len(df)\n",
    "\n",
    "    print(f\"  â€¢ Urgent Documents: {urgent_docs} ({urgent_docs/total_docs*100:.1f}%)\")\n",
    "    print(f\"  â€¢ Non-Urgent Documents: {non_urgent_docs} ({non_urgent_docs/total_docs*100:.1f}%)\")\n",
    "\n",
    "    # Status analysis\n",
    "    print(f\"\\nâœ… Status Analysis:\")\n",
    "    status_counts = df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} documents\")\n",
    "\n",
    "    # Show sample urgency analyses as markdown table for judges\n",
    "    print(f\"\\nğŸ“ Sample Urgency Analyses:\")\n",
    "    print(f\"\\n## AI.GENERATE_BOOL Results - Legal Document Urgency Detection\")\n",
    "    print(f\"\\n| Document ID | Type | Urgency | Status | AI Response |\")\n",
    "    print(f\"|-------------|------|---------|--------|-------------|\")\n",
    "\n",
    "    for i, row in df.head(5).iterrows():\n",
    "        urgency_icon = \"ğŸš¨\" if row['is_urgent'] else \"âœ…\"\n",
    "        urgency_status = \"URGENT\" if row['is_urgent'] else \"Non-Urgent\"\n",
    "        urgency_text = str(row['urgency_text'])[:30] + '...' if len(str(row['urgency_text'])) > 30 else str(row['urgency_text'])\n",
    "\n",
    "        print(f\"| {row['document_id']} | {row['document_type']} | {urgency_icon} {urgency_status} | {row['status']} | {urgency_text} |\")\n",
    "\n",
    "    print(f\"\\n**Urgency Summary:**\")\n",
    "    print(f\"- Total Documents: {len(df)}\")\n",
    "    print(f\"- Urgent Documents: {urgent_docs} ({urgent_docs/total_docs*100:.1f}%)\")\n",
    "    print(f\"- Non-Urgent Documents: {non_urgent_docs} ({non_urgent_docs/total_docs*100:.1f}%)\")\n",
    "    print(f\"- Processing Time: {result['processing_time']:.2f} seconds\")\n",
    "\n",
    "    # Calculate business impact\n",
    "    print(f\"\\nğŸ’¼ Business Impact Analysis:\")\n",
    "    print(f\"Time Saved per Document: ~5 minutes (manual review) vs {result['avg_time_per_doc']:.2f}s (AI)\")\n",
    "    time_saved_per_doc = 5 * 60 - result['avg_time_per_doc']  # 5 minutes in seconds\n",
    "    total_time_saved = time_saved_per_doc * len(df)\n",
    "    print(f\"Total Time Saved: {total_time_saved/60:.1f} minutes for {len(df)} documents\")\n",
    "    print(f\"Efficiency Improvement: {(time_saved_per_doc / (5*60)) * 100:.1f}%\")\n",
    "\n",
    "    # Urgency detection value\n",
    "    if urgent_docs > 0:\n",
    "        print(f\"\\nğŸ¯ Urgency Detection Value:\")\n",
    "        print(f\"  â€¢ {urgent_docs} urgent documents identified for immediate attention\")\n",
    "        print(f\"  â€¢ Potential to prevent missed deadlines and legal issues\")\n",
    "        print(f\"  â€¢ Improved case prioritization and resource allocation\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run analysis\n",
    "if 'bool_result' in locals() and isinstance(bool_result, dict) and 'urgency_analyses' in bool_result:\n",
    "    df_urgency = analyze_urgency_results(bool_result)\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for analysis. Please run ai_generate_bool() first.\")\n",
    "    print(\"ğŸ’¡ Tip: Make sure to run the ai_generate_bool() function to get results for analysis.\")"
   ],
   "id": "ca33b165-9bae-48a4-a6fa-d132ae8b700c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI.GENERATE_BOOL Quality Assessment**\n",
    "\n",
    "Letâ€™s show the original document content alongside the urgency\n",
    "classification for quality evaluation:"
   ],
   "id": "aa43e4eb-971d-4d1f-b572-162ff8136100"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original content vs urgency classification for quality assessment\n",
    "def show_content_vs_urgency(result):\n",
    "    \"\"\"Show original document content alongside urgency classification.\"\"\"\n",
    "\n",
    "    if not result or 'urgency_analyses' not in result:\n",
    "        print(\"âš ï¸  No results available for content comparison\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸ” Content vs Urgency Classification Quality Assessment\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Get original content for comparison\n",
    "    for i, urgency_data in enumerate(result['urgency_analyses'][:2], 1):  # Show first 2 for detailed review\n",
    "        doc_id = urgency_data['document_id']\n",
    "\n",
    "        # Get original content\n",
    "        content_query = f\"\"\"\n",
    "        SELECT content, document_type, metadata\n",
    "        FROM `{config['project']['id']}.legal_ai_platform_raw_data.legal_documents`\n",
    "        WHERE document_id = '{doc_id}'\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            content_result = client.query(content_query).result()\n",
    "            original_doc = next(content_result)\n",
    "\n",
    "            urgency_icon = \"ğŸš¨\" if urgency_data['is_urgent'] else \"âœ…\"\n",
    "            urgency_status = \"URGENT\" if urgency_data['is_urgent'] else \"Non-Urgent\"\n",
    "\n",
    "            print(f\"\\n{'='*100}\")\n",
    "            print(f\"{urgency_icon} DOCUMENT {i}: {doc_id} ({urgency_data['document_type']})\")\n",
    "            print(f\"{'='*100}\")\n",
    "\n",
    "            print(f\"\\nğŸ“„ ORIGINAL CONTENT (First 500 characters):\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            print(f\"{original_doc.content[:500]}...\")\n",
    "            print(f\"\\n[Total Length: {len(original_doc.content):,} characters]\")\n",
    "\n",
    "            print(f\"\\nğŸ¤– AI URGENCY CLASSIFICATION:\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            print(f\"Urgency Status: {urgency_status}\")\n",
    "            print(f\"AI Response: {urgency_data['urgency_text']}\")\n",
    "            print(f\"Boolean Result: {urgency_data['is_urgent']}\")\n",
    "\n",
    "            print(f\"\\nğŸ“Š URGENCY ANALYSIS:\")\n",
    "            print(f\"  â€¢ Original Length: {len(original_doc.content):,} characters\")\n",
    "            print(f\"  â€¢ Urgency Classification: {urgency_status}\")\n",
    "            print(f\"  â€¢ AI Confidence: {urgency_data['urgency_text']}\")\n",
    "            print(f\"  â€¢ Processing Status: {urgency_data['status']}\")\n",
    "\n",
    "            # Analyze content for urgency indicators\n",
    "            urgency_keywords = ['deadline', 'urgent', 'immediate', 'emergency', 'time-sensitive', 'expires', 'due date', 'asap']\n",
    "            content_lower = original_doc.content.lower()\n",
    "            found_keywords = [keyword for keyword in urgency_keywords if keyword in content_lower]\n",
    "\n",
    "            if found_keywords:\n",
    "                print(f\"\\nğŸ” URGENCY INDICATORS FOUND:\")\n",
    "                for keyword in found_keywords:\n",
    "                    print(f\"  â€¢ '{keyword}' detected in content\")\n",
    "            else:\n",
    "                print(f\"\\nğŸ” NO OBVIOUS URGENCY INDICATORS FOUND\")\n",
    "\n",
    "            if original_doc.metadata:\n",
    "                print(f\"\\nğŸ“‹ METADATA:\")\n",
    "                print(f\"  {original_doc.metadata}\")\n",
    "\n",
    "            print(f\"{'='*100}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to get original content for {doc_id}: {e}\")\n",
    "\n",
    "    print(f\"\\nâœ… Quality Assessment Complete\")\n",
    "\n",
    "# Run content vs urgency comparison\n",
    "if 'bool_result' in locals() and isinstance(bool_result, dict) and 'urgency_analyses' in bool_result:\n",
    "    show_content_vs_urgency(bool_result)\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for content comparison. Please run ai_generate_bool() first.\")"
   ],
   "id": "38157ba1-0abb-47e5-b908-46febff7b4b6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4 AI.FORECAST - Case Outcome Prediction**\n",
    "\n",
    "Letâ€™s implement the AI.FORECAST function to predict case outcomes using\n",
    "BigQuery AI. This demonstrates how we can use historical legal data to\n",
    "forecast future case results and provide strategic insights."
   ],
   "id": "513a0944-2acf-45ed-bb81-4a10febe665d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_forecast(case_type=\"case_law\", limit=10):\n",
    "    \"\"\"\n",
    "    Implement ML.FORECAST for case outcome prediction using BigQuery AI time-series model.\n",
    "\n",
    "    Args:\n",
    "        case_type: Type of case to forecast (default: \"case_law\")\n",
    "        limit: Number of historical data points to use (default: 10)\n",
    "\n",
    "    Returns:\n",
    "        Dict containing forecast results\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸš€ Starting ML.FORECAST outcome prediction...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Connect to BigQuery\n",
    "        if not client:\n",
    "            raise Exception(\"BigQuery client not initialized\")\n",
    "\n",
    "        # Build parameterized query for time-series forecasting\n",
    "        # Note: ARIMA_PLUS models don't support the third parameter (data subquery)\n",
    "        # The model is trained on historical data during creation\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "            forecast_timestamp,\n",
    "            forecast_value,\n",
    "            standard_error,\n",
    "            confidence_level,\n",
    "            confidence_interval_lower_bound,\n",
    "            confidence_interval_upper_bound\n",
    "        FROM ML.FORECAST(\n",
    "            MODEL `{project_id}.ai_models.legal_timesfm`,\n",
    "            STRUCT(7 AS horizon, 0.95 AS confidence_level)\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        # Format query with project ID\n",
    "        query = query.format(project_id=config['project']['id'])\n",
    "\n",
    "        print(\"ğŸ“ Executing ML.FORECAST query...\")\n",
    "        result = client.query(query)\n",
    "\n",
    "        # Process results\n",
    "        forecasts = []\n",
    "        for row in result:\n",
    "            forecast_data = {\n",
    "                'case_type': case_type,\n",
    "                'forecast_timestamp': row.forecast_timestamp.isoformat(),\n",
    "                'forecast_value': row.forecast_value,\n",
    "                'standard_error': row.standard_error,\n",
    "                'confidence_level': row.confidence_level,\n",
    "                'confidence_interval_lower': row.confidence_interval_lower_bound,\n",
    "                'confidence_interval_upper': row.confidence_interval_upper_bound,\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            }\n",
    "            forecasts.append(forecast_data)\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        print(f\"âœ… Generated {len(forecasts)} outcome forecasts using ML.FORECAST\")\n",
    "        print(f\"â±ï¸  Processing time: {processing_time:.2f} seconds\")\n",
    "\n",
    "        return {\n",
    "            'function': 'AI.FORECAST',\n",
    "            'purpose': 'Case Outcome Prediction',\n",
    "            'total_forecasts': len(forecasts),\n",
    "            'forecasts': forecasts,\n",
    "            'processing_time': processing_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ML.FORECAST outcome prediction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test the function and store results for analysis\n",
    "print(\"ğŸ§ª Testing ML.FORECAST function...\")\n",
    "try:\n",
    "    # Run ML.FORECAST and store results\n",
    "    ai_forecast_result = ai_forecast(\"case_law\", 1)\n",
    "    print(f\"âœ… Function test successful!\")\n",
    "    print(f\"ğŸ“ˆ Generated {ai_forecast_result['total_forecasts']} forecasts\")\n",
    "    print(f\"âš¡ Processing time: {ai_forecast_result['processing_time']:.2f}s\")\n",
    "\n",
    "    # Store result for analysis functions\n",
    "    forecast_result = ai_forecast_result\n",
    "    print(f\"ğŸ’¾ Results stored in 'forecast_result' variable for analysis\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Function test failed: {e}\")\n",
    "    print(f\"ğŸ’¡ Make sure BigQuery client is connected and time-series model is available\")"
   ],
   "id": "1ba3743e-dcc6-4dad-a99d-125ffc4ac545"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI.FORECAST Results Analysis**\n",
    "\n",
    "Letâ€™s analyze the case outcome prediction results and demonstrate the\n",
    "strategic value:"
   ],
   "id": "770b57f7-c59a-466f-8401-248ad3eb6e96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ML.FORECAST results\n",
    "def analyze_forecast_results(result):\n",
    "    \"\"\"Analyze and visualize ML.FORECAST results.\"\"\"\n",
    "\n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(result['forecasts'])\n",
    "\n",
    "    print(\"ğŸ“Š ML.FORECAST Results Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Basic statistics\n",
    "    print(f\"Total Forecasts Generated: {len(df)}\")\n",
    "    print(f\"Processing Time: {result['processing_time']:.2f} seconds\")\n",
    "\n",
    "    # Case type distribution\n",
    "    print(f\"\\nğŸ“‹ Case Type Distribution:\")\n",
    "    case_types = df['case_type'].value_counts()\n",
    "    for case_type, count in case_types.items():\n",
    "        print(f\"  {case_type}: {count} forecasts\")\n",
    "\n",
    "    # Forecast value analysis\n",
    "    print(f\"\\nğŸ“ˆ Forecast Value Analysis:\")\n",
    "    print(f\"  â€¢ Average Forecast Value: {df['forecast_value'].mean():.2f}\")\n",
    "    print(f\"  â€¢ Min Forecast Value: {df['forecast_value'].min():.2f}\")\n",
    "    print(f\"  â€¢ Max Forecast Value: {df['forecast_value'].max():.2f}\")\n",
    "    print(f\"  â€¢ Standard Deviation: {df['forecast_value'].std():.2f}\")\n",
    "\n",
    "    # Confidence interval analysis\n",
    "    print(f\"\\nğŸ“Š Confidence Interval Analysis:\")\n",
    "    print(f\"  â€¢ Average Confidence Level: {df['confidence_level'].mean():.3f}\")\n",
    "    print(f\"  â€¢ Average Standard Error: {df['standard_error'].mean():.2f}\")\n",
    "    print(f\"  â€¢ Average Lower Bound: {df['confidence_interval_lower'].mean():.2f}\")\n",
    "    print(f\"  â€¢ Average Upper Bound: {df['confidence_interval_upper'].mean():.2f}\")\n",
    "\n",
    "    # Show sample forecasts\n",
    "    print(f\"\\nğŸ“ Sample Forecasts:\")\n",
    "    for i, row in df.head(3).iterrows():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ“… Forecast {i+1}: {row['case_type']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Forecast Timestamp: {row['forecast_timestamp']}\")\n",
    "        print(f\"Forecast Value: {row['forecast_value']:.2f}\")\n",
    "        print(f\"Standard Error: {row['standard_error']:.2f}\")\n",
    "        print(f\"Confidence Level: {row['confidence_level']:.3f}\")\n",
    "        print(f\"Confidence Interval: [{row['confidence_interval_lower']:.2f}, {row['confidence_interval_upper']:.2f}]\")\n",
    "        print(f\"Created: {row['created_at']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "    # Calculate business impact\n",
    "    print(f\"\\nğŸ’¼ Business Impact Analysis:\")\n",
    "    print(f\"Time Saved per Forecast: ~2 hours (manual analysis) vs {result['processing_time']:.2f}s (AI)\")\n",
    "    time_saved_per_forecast = 2 * 60 * 60 - result['processing_time']  # 2 hours in seconds\n",
    "    total_time_saved = time_saved_per_forecast * len(df)\n",
    "    print(f\"Total Time Saved: {total_time_saved/3600:.1f} hours for {len(df)} forecasts\")\n",
    "    print(f\"Efficiency Improvement: {(time_saved_per_forecast / (2*60*60)) * 100:.1f}%\")\n",
    "\n",
    "    # Strategic value analysis\n",
    "    avg_confidence = df['confidence_level'].mean()\n",
    "    forecast_trend = \"Increasing\" if df['forecast_value'].iloc[-1] > df['forecast_value'].iloc[0] else \"Decreasing\"\n",
    "\n",
    "    print(f\"\\nğŸ¯ Strategic Value Analysis:\")\n",
    "    print(f\"  â€¢ {len(df)} time-series forecasts generated\")\n",
    "    print(f\"  â€¢ Average confidence level: {avg_confidence:.1%}\")\n",
    "    print(f\"  â€¢ Forecast trend: {forecast_trend}\")\n",
    "    print(f\"  â€¢ Potential for case volume planning and resource allocation\")\n",
    "    print(f\"  â€¢ Enhanced strategic decision-making with predictive insights\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run analysis\n",
    "if 'forecast_result' in locals() and isinstance(forecast_result, dict) and 'forecasts' in forecast_result:\n",
    "    df_forecast = analyze_forecast_results(forecast_result)\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for analysis. Please run ai_forecast() first.\")\n",
    "    print(\"ğŸ’¡ Tip: Make sure to run the ai_forecast() function to get results for analysis.\")"
   ],
   "id": "1a1fa874-a091-4276-94bf-d09620cc85c2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI.FORECAST Quality Assessment**\n",
    "\n",
    "Letâ€™s show the original document content alongside the outcome\n",
    "prediction for quality evaluation:"
   ],
   "id": "ba1d67f0-08cf-49cd-9337-bc4ab1687236"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show forecast results for quality assessment\n",
    "def show_forecast_quality_assessment(result):\n",
    "    \"\"\"Show ML.FORECAST results for quality assessment.\"\"\"\n",
    "\n",
    "    if not result or 'forecasts' not in result:\n",
    "        print(\"âš ï¸  No results available for forecast assessment\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸ” ML.FORECAST Quality Assessment\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Show forecast details\n",
    "    for i, forecast_data in enumerate(result['forecasts'][:3], 1):  # Show first 3 forecasts\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"ğŸ“… FORECAST {i}: {forecast_data['case_type']}\")\n",
    "        print(f\"{'='*100}\")\n",
    "\n",
    "        print(f\"\\nğŸ“Š FORECAST DETAILS:\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        print(f\"Forecast Timestamp: {forecast_data['forecast_timestamp']}\")\n",
    "        print(f\"Forecast Value: {forecast_data['forecast_value']:.2f}\")\n",
    "        print(f\"Standard Error: {forecast_data['standard_error']:.2f}\")\n",
    "        print(f\"Confidence Level: {forecast_data['confidence_level']:.3f}\")\n",
    "        print(f\"Confidence Interval: [{forecast_data['confidence_interval_lower']:.2f}, {forecast_data['confidence_interval_upper']:.2f}]\")\n",
    "\n",
    "        print(f\"\\nğŸ“ˆ FORECAST ANALYSIS:\")\n",
    "        print(f\"  â€¢ Forecast Value: {forecast_data['forecast_value']:.2f} cases\")\n",
    "        print(f\"  â€¢ Confidence Level: {forecast_data['confidence_level']:.1%}\")\n",
    "        print(f\"  â€¢ Standard Error: {forecast_data['standard_error']:.2f}\")\n",
    "        print(f\"  â€¢ Interval Width: {forecast_data['confidence_interval_upper'] - forecast_data['confidence_interval_lower']:.2f}\")\n",
    "        print(f\"  â€¢ Created: {forecast_data['created_at']}\")\n",
    "\n",
    "        # Analyze forecast quality\n",
    "        confidence_width = forecast_data['confidence_interval_upper'] - forecast_data['confidence_interval_lower']\n",
    "        relative_error = forecast_data['standard_error'] / forecast_data['forecast_value'] if forecast_data['forecast_value'] > 0 else 0\n",
    "\n",
    "        print(f\"\\nğŸ” FORECAST QUALITY INDICATORS:\")\n",
    "        print(f\"  â€¢ Relative Error: {relative_error:.1%}\")\n",
    "        print(f\"  â€¢ Confidence Interval Width: {confidence_width:.2f}\")\n",
    "        print(f\"  â€¢ Model Confidence: {forecast_data['confidence_level']:.1%}\")\n",
    "\n",
    "        if relative_error < 0.1:\n",
    "            print(f\"  â€¢ Quality Assessment: âœ… High Precision\")\n",
    "        elif relative_error < 0.2:\n",
    "            print(f\"  â€¢ Quality Assessment: ğŸŸ¡ Medium Precision\")\n",
    "        else:\n",
    "            print(f\"  â€¢ Quality Assessment: ğŸ”´ Low Precision\")\n",
    "\n",
    "        print(f\"{'='*100}\")\n",
    "\n",
    "    print(f\"\\nâœ… Quality Assessment Complete\")\n",
    "\n",
    "# Run forecast quality assessment\n",
    "if 'forecast_result' in locals() and isinstance(forecast_result, dict) and 'forecasts' in forecast_result:\n",
    "    show_forecast_quality_assessment(forecast_result)\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for forecast assessment. Please run ai_forecast() first.\")"
   ],
   "id": "b49abdb0-4984-44b1-a027-d38b1db18ad0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 5: Track 2 - Vector Search Functions**\n",
    "\n",
    "Now letâ€™s implement the Track 2 Vector Search functions to demonstrate\n",
    "BigQueryâ€™s advanced vector capabilities for semantic search and\n",
    "similarity analysis in legal documents."
   ],
   "id": "b240738c-1f7d-4f0b-9723-951301bc2e81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1 ML.GENERATE_EMBEDDING - Document Embeddings**\n",
    "\n",
    "Letâ€™s implement the ML.GENERATE_EMBEDDING function to create vector\n",
    "embeddings for legal documents, enabling semantic search and similarity\n",
    "analysis."
   ],
   "id": "115d5bda-ce19-4b13-b21a-5e504aacd0f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_generate_embedding(document_id=None, limit=10):\n",
    "    \"\"\"\n",
    "    Implement ML.GENERATE_EMBEDDING for document embeddings using BigQuery AI.\n",
    "\n",
    "    Args:\n",
    "        document_id: Specific document ID to embed (optional)\n",
    "        limit: Number of documents to process (default: 10)\n",
    "\n",
    "    Returns:\n",
    "        Dict containing embedding results\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸš€ Starting ML.GENERATE_EMBEDDING...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Connect to BigQuery\n",
    "        if not client:\n",
    "            raise Exception(\"BigQuery client not initialized\")\n",
    "\n",
    "        # Build query using actual ML.GENERATE_EMBEDDING function\n",
    "        if document_id:\n",
    "            where_clause = f\"WHERE document_id = '{document_id}'\"\n",
    "        else:\n",
    "            where_clause = f\"ORDER BY created_at DESC LIMIT {limit}\"\n",
    "\n",
    "        # Use actual BigQuery AI function - ML.GENERATE_EMBEDDING as TVF with pre-built model\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            document_id,\n",
    "            document_type,\n",
    "            ml_generate_embedding_result AS embedding,\n",
    "            ml_generate_embedding_status AS status\n",
    "        FROM ML.GENERATE_EMBEDDING(\n",
    "            MODEL `{config['project']['id']}.ai_models.text_embedding`,\n",
    "            (\n",
    "                SELECT\n",
    "                    document_id,\n",
    "                    document_type,\n",
    "                    content\n",
    "                FROM `{config['project']['id']}.legal_ai_platform_raw_data.legal_documents`\n",
    "                {where_clause}\n",
    "            )\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"ğŸ“ Executing ML.GENERATE_EMBEDDING query...\")\n",
    "        result = client.query(query)\n",
    "\n",
    "        # Process results\n",
    "        embeddings = []\n",
    "        for row in result:\n",
    "            embedding_data = {\n",
    "                'document_id': row.document_id,\n",
    "                'document_type': row.document_type,\n",
    "                'embedding': row.embedding,\n",
    "                'embedding_dimension': len(row.embedding) if row.embedding else 0,\n",
    "                'status': row.status or \"OK\",\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            }\n",
    "            embeddings.append(embedding_data)\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        print(f\"âœ… Generated {len(embeddings)} document embeddings using ML.GENERATE_EMBEDDING\")\n",
    "        print(f\"â±ï¸  Processing time: {processing_time:.2f} seconds\")\n",
    "        print(f\"ğŸ“Š Average time per document: {processing_time/len(embeddings):.2f} seconds\")\n",
    "\n",
    "        return {\n",
    "            'function': 'ML.GENERATE_EMBEDDING',\n",
    "            'purpose': 'Document Embeddings',\n",
    "            'total_documents': len(embeddings),\n",
    "            'embeddings': embeddings,\n",
    "            'processing_time': processing_time,\n",
    "            'avg_time_per_doc': processing_time/len(embeddings),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ML.GENERATE_EMBEDDING failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test the function and store results for analysis\n",
    "print(\"ğŸ§ª Testing ML.GENERATE_EMBEDDING function...\")\n",
    "try:\n",
    "    # Run ML.GENERATE_EMBEDDING and store results\n",
    "    ml_generate_embedding_result = ml_generate_embedding(limit=3)\n",
    "    print(f\"âœ… Function test successful!\")\n",
    "    print(f\"ğŸ“ˆ Generated {ml_generate_embedding_result['total_documents']} embeddings\")\n",
    "    print(f\"âš¡ Average processing time: {ml_generate_embedding_result['avg_time_per_doc']:.2f}s per document\")\n",
    "\n",
    "    # Store result for analysis functions\n",
    "    embedding_result = ml_generate_embedding_result\n",
    "    print(f\"ğŸ’¾ Results stored in 'embedding_result' variable for analysis\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Function test failed: {e}\")\n",
    "    print(f\"ğŸ’¡ Make sure BigQuery client is connected and embedding model is available\")"
   ],
   "id": "c3283d61-7cc3-4324-952c-e063ec71d77a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ML.GENERATE_EMBEDDING Results Analysis**\n",
    "\n",
    "Letâ€™s analyze the embedding generation results and demonstrate the\n",
    "vector capabilities:"
   ],
   "id": "4e5f985f-9c91-45f6-8014-1fab0caf0306"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ML.GENERATE_EMBEDDING results\n",
    "def analyze_embedding_results(result):\n",
    "    \"\"\"Analyze and visualize ML.GENERATE_EMBEDDING results.\"\"\"\n",
    "\n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(result['embeddings'])\n",
    "\n",
    "    print(\"ğŸ“Š ML.GENERATE_EMBEDDING Results Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Basic statistics\n",
    "    print(f\"Total Documents Processed: {len(df)}\")\n",
    "    print(f\"Processing Time: {result['processing_time']:.2f} seconds\")\n",
    "    print(f\"Average Time per Document: {result['avg_time_per_doc']:.2f} seconds\")\n",
    "\n",
    "    # Document type distribution\n",
    "    print(f\"\\nğŸ“‹ Document Type Distribution:\")\n",
    "    doc_types = df['document_type'].value_counts()\n",
    "    for doc_type, count in doc_types.items():\n",
    "        print(f\"  {doc_type}: {count} documents\")\n",
    "\n",
    "    # Embedding dimension analysis\n",
    "    print(f\"\\nğŸ”¢ Embedding Dimension Analysis:\")\n",
    "    embedding_dims = df['embedding_dimension'].value_counts()\n",
    "    for dim, count in embedding_dims.items():\n",
    "        print(f\"  {dim} dimensions: {count} documents\")\n",
    "\n",
    "    # Status analysis\n",
    "    print(f\"\\nâœ… Status Analysis:\")\n",
    "    status_counts = df['status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"  {status}: {count} documents\")\n",
    "\n",
    "    # Show sample embeddings\n",
    "    print(f\"\\nğŸ“ Sample Embeddings:\")\n",
    "    for i, row in df.head(3).iterrows():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Document {row['document_id']} ({row['document_type']})\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Embedding Dimension: {row['embedding_dimension']}\")\n",
    "        print(f\"First 5 Values: {row['embedding'][:5] if row['embedding'] else 'None'}\")\n",
    "        print(f\"Last 5 Values: {row['embedding'][-5:] if row['embedding'] else 'None'}\")\n",
    "        print(f\"Status: {row['status']}\")\n",
    "        print(f\"Created: {row['created_at']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "    # Calculate business impact\n",
    "    print(f\"\\nğŸ’¼ Business Impact Analysis:\")\n",
    "    print(f\"Time Saved per Document: ~2 minutes (manual processing) vs {result['avg_time_per_doc']:.2f}s (AI)\")\n",
    "    time_saved_per_doc = 2 * 60 - result['avg_time_per_doc']  # 2 minutes in seconds\n",
    "    total_time_saved = time_saved_per_doc * len(df)\n",
    "    print(f\"Total Time Saved: {total_time_saved/60:.1f} minutes for {len(df)} documents\")\n",
    "    print(f\"Efficiency Improvement: {(time_saved_per_doc / (2*60)) * 100:.1f}%\")\n",
    "\n",
    "    # Vector search value\n",
    "    print(f\"\\nğŸ¯ Vector Search Value:\")\n",
    "    print(f\"  â€¢ {len(df)} documents now have vector representations\")\n",
    "    print(f\"  â€¢ Enables semantic similarity search across legal documents\")\n",
    "    print(f\"  â€¢ Supports advanced document retrieval and clustering\")\n",
    "    print(f\"  â€¢ Foundation for intelligent legal research and case law discovery\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run analysis\n",
    "if 'embedding_result' in locals() and isinstance(embedding_result, dict) and 'embeddings' in embedding_result:\n",
    "    df_embeddings = analyze_embedding_results(embedding_result)\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for analysis. Please run ml_generate_embedding() first.\")\n",
    "    print(\"ğŸ’¡ Tip: Make sure to run the ml_generate_embedding() function to get results for analysis.\")"
   ],
   "id": "f57d49d8-729d-42e6-ac5c-ca018359d619"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 VECTOR_SEARCH - Semantic Similarity Search**\n",
    "\n",
    "Letâ€™s implement the VECTOR_SEARCH function to find semantically similar\n",
    "legal documents using vector embeddings."
   ],
   "id": "f21005ae-181e-4636-a9c7-52b9fa07ba82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query_text, limit=10):\n",
    "    \"\"\"\n",
    "    Implement VECTOR_SEARCH for similarity search using BigQuery AI.\n",
    "\n",
    "    Args:\n",
    "        query_text: Text to search for similar documents\n",
    "        limit: Number of results to return (default: 10)\n",
    "\n",
    "    Returns:\n",
    "        Dict containing search results\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸš€ Starting VECTOR_SEARCH for query: {query_text[:50]}...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not client:\n",
    "            raise Exception(\"BigQuery client not initialized\")\n",
    "\n",
    "        # First, we need to ensure we have embeddings in the embeddings table\n",
    "        # Check if embeddings table exists and has data\n",
    "        check_query = f\"\"\"\n",
    "        SELECT COUNT(*) as row_count\n",
    "        FROM `{config['project']['id']}.legal_ai_platform_vector_indexes.document_embeddings`\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            check_result = client.query(check_query)\n",
    "            row_count = list(check_result)[0].row_count\n",
    "            if row_count == 0:\n",
    "                print(\"âš ï¸  No embeddings found in embeddings table. Generating embeddings first...\")\n",
    "                # Generate embeddings for a few documents\n",
    "                embedding_result = ml_generate_embedding(limit=5)\n",
    "                print(\"âœ… Embeddings generated. Please run vector_search again.\")\n",
    "                return {\n",
    "                    'function': 'VECTOR_SEARCH',\n",
    "                    'purpose': 'Similarity Search',\n",
    "                    'message': 'Embeddings generated. Please run vector_search again.',\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Embeddings table not found or accessible: {e}\")\n",
    "            print(\"ğŸ’¡ Please ensure embeddings are generated first using ml_generate_embedding()\")\n",
    "            return {\n",
    "                'function': 'VECTOR_SEARCH',\n",
    "                'purpose': 'Similarity Search',\n",
    "                'error': 'Embeddings table not available',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "        # Build VECTOR_SEARCH query\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            base.document_id,\n",
    "            distance AS similarity_distance\n",
    "        FROM VECTOR_SEARCH(\n",
    "            (\n",
    "                SELECT\n",
    "                    document_id,\n",
    "                    embedding\n",
    "                FROM `{config['project']['id']}.legal_ai_platform_vector_indexes.document_embeddings`\n",
    "                WHERE embedding IS NOT NULL\n",
    "            ),\n",
    "            'embedding',\n",
    "            (\n",
    "                SELECT\n",
    "                    ml_generate_embedding_result AS query_embedding\n",
    "                FROM ML.GENERATE_EMBEDDING(\n",
    "                    MODEL `{config['project']['id']}.ai_models.text_embedding`,\n",
    "                    (SELECT '{query_text}' AS content)\n",
    "                )\n",
    "                WHERE ml_generate_embedding_status = ''\n",
    "            ),\n",
    "            top_k => {limit},\n",
    "            distance_type => 'COSINE'\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"ğŸ“ Executing VECTOR_SEARCH query...\")\n",
    "        result = client.query(query)\n",
    "\n",
    "        # Process results\n",
    "        search_results = []\n",
    "        for row in result:\n",
    "            result_data = {\n",
    "                'document_id': row.document_id,\n",
    "                'similarity_distance': row.similarity_distance,\n",
    "                'similarity_score': 1 - row.similarity_distance,  # Convert distance to similarity score\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            }\n",
    "            search_results.append(result_data)\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        print(f\"âœ… Generated {len(search_results)} vector search results\")\n",
    "        print(f\"â±ï¸  Processing time: {processing_time:.2f} seconds\")\n",
    "\n",
    "        return {\n",
    "            'function': 'VECTOR_SEARCH',\n",
    "            'purpose': 'Similarity Search',\n",
    "            'query_text': query_text,\n",
    "            'total_results': len(search_results),\n",
    "            'results': search_results,\n",
    "            'processing_time': processing_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ VECTOR_SEARCH failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test the function with targeted legal queries to demonstrate different similarity levels\n",
    "print(\"ğŸ§ª Testing VECTOR_SEARCH function with targeted queries...\")\n",
    "\n",
    "# Test multiple queries to showcase different similarity levels\n",
    "# Using actual terms from the legal documents for better matching\n",
    "test_queries = [\n",
    "    (\"marriage licenses\", \"High similarity - exact term from Don Davis case\"),\n",
    "    (\"writ of mandamus\", \"High similarity - exact legal term from Scottsdale case\"),\n",
    "    (\"breach of contract\", \"High similarity - exact term from Scottsdale case\"),\n",
    "    (\"probate judge\", \"High similarity - exact role from Don Davis case\"),\n",
    "    (\"search seizure\", \"Medium-high similarity - from Melton case\"),\n",
    "    (\"sheriff corruption\", \"Medium-high similarity - from Clark case\"),\n",
    "    (\"arbitration program\", \"Medium similarity - from Scheehle case\"),\n",
    "    (\"election petition\", \"Medium similarity - from Haney case\"),\n",
    "    (\"court rules\", \"Lower similarity - general legal concept\")\n",
    "]\n",
    "\n",
    "search_results = {}\n",
    "\n",
    "for query_text, description in test_queries:\n",
    "    print(f\"\\nğŸ” Testing: '{query_text}' ({description})\")\n",
    "    try:\n",
    "        result = vector_search(query_text, limit=3)\n",
    "        search_results[query_text] = result\n",
    "\n",
    "        if 'results' in result:\n",
    "            avg_similarity = sum(r['similarity_score'] for r in result['results']) / len(result['results'])\n",
    "            print(f\"âœ… Found {result['total_results']} results, avg similarity: {avg_similarity:.3f}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  {result.get('error', result.get('message', 'No results'))}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Query failed: {e}\")\n",
    "\n",
    "# Store the best result for detailed analysis\n",
    "if search_results:\n",
    "    best_query = max(search_results.keys(),\n",
    "                    key=lambda q: sum(r['similarity_score'] for r in search_results[q]['results']) / len(search_results[q]['results'])\n",
    "                    if 'results' in search_results[q] else 0)\n",
    "    search_result = search_results[best_query]\n",
    "    print(f\"\\nğŸ’¾ Best result stored in 'search_result' variable: '{best_query}'\")\n",
    "else:\n",
    "    print(\"âš ï¸  No successful searches completed\")"
   ],
   "id": "5c0df348-65c6-4136-abde-5edfbd3cc8bb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VECTOR_SEARCH Results Analysis**\n",
    "\n",
    "Letâ€™s analyze the similarity search results and demonstrate the semantic\n",
    "search capabilities:"
   ],
   "id": "15541c75-9a0b-45e5-a39c-4a3eaaeb58b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified VECTOR_SEARCH and ML.DISTANCE Analysis\n",
    "def analyze_vector_search_results(result):\n",
    "    \"\"\"Simplified analysis of VECTOR_SEARCH results.\"\"\"\n",
    "\n",
    "    if 'error' in result or 'message' in result:\n",
    "        print(\"âš ï¸  VECTOR_SEARCH not available or embeddings not ready\")\n",
    "        print(f\"Status: {result.get('error', result.get('message', 'Unknown'))}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(result['results'])\n",
    "\n",
    "    print(\"ğŸ“Š VECTOR_SEARCH Results Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Basic metrics\n",
    "    print(f\"Query: '{result['query_text']}'\")\n",
    "    print(f\"Results Found: {len(df)}\")\n",
    "    print(f\"Processing Time: {result['processing_time']:.2f}s\")\n",
    "    print(f\"Average Similarity: {df['similarity_score'].mean():.3f}\")\n",
    "    print(f\"Best Match: {df['similarity_score'].max():.3f}\")\n",
    "\n",
    "    # Show top 3 results\n",
    "    print(f\"\\nğŸ“ Top Results:\")\n",
    "    for i, row in df.head(3).iterrows():\n",
    "        similarity_level = \"High\" if row['similarity_score'] > 0.7 else \"Medium\" if row['similarity_score'] > 0.5 else \"Low\"\n",
    "        print(f\"  {i+1}. {row['document_id']} - {row['similarity_score']:.3f} ({similarity_level})\")\n",
    "\n",
    "    # Business impact\n",
    "    manual_time = 30 * 60  # 30 minutes\n",
    "    ai_time = result['processing_time']\n",
    "    time_saved = (manual_time - ai_time) / 60\n",
    "    print(f\"\\nğŸ’¼ Business Impact:\")\n",
    "    print(f\"Time Saved: {time_saved:.1f} minutes per search\")\n",
    "    print(f\"Efficiency: {manual_time/ai_time:.0f}x faster than manual research\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def vector_distance_analysis(doc1_id, doc2_id):\n",
    "    \"\"\"Analyze ML.DISTANCE between two documents.\"\"\"\n",
    "\n",
    "    print(f\"ğŸ” ML.DISTANCE Analysis: {doc1_id} vs {doc2_id}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # Calculate ML.DISTANCE using BigQuery with cosine similarity\n",
    "        distance_query = f\"\"\"\n",
    "        SELECT\n",
    "            ML.DISTANCE(\n",
    "                (SELECT embedding FROM `{config['project']['id']}.legal_ai_platform_vector_indexes.document_embeddings` WHERE document_id = '{doc1_id}'),\n",
    "                (SELECT embedding FROM `{config['project']['id']}.legal_ai_platform_vector_indexes.document_embeddings` WHERE document_id = '{doc2_id}'),\n",
    "                'COSINE'\n",
    "            ) AS cosine_distance,\n",
    "            -- Calculate similarity score (1 - distance for cosine)\n",
    "            (1 - ML.DISTANCE(\n",
    "                (SELECT embedding FROM `{config['project']['id']}.legal_ai_platform_vector_indexes.document_embeddings` WHERE document_id = '{doc1_id}'),\n",
    "                (SELECT embedding FROM `{config['project']['id']}.legal_ai_platform_vector_indexes.document_embeddings` WHERE document_id = '{doc2_id}'),\n",
    "                'COSINE'\n",
    "            )) AS cosine_similarity\n",
    "        \"\"\"\n",
    "\n",
    "        distance_result = client.query(distance_query)\n",
    "        distance_row = next(distance_result.result())\n",
    "        cosine_distance = distance_row.cosine_distance\n",
    "        similarity = distance_row.cosine_similarity  # Use direct similarity from BigQuery\n",
    "\n",
    "        print(f\"ğŸ“Š Distance Metrics:\")\n",
    "        print(f\"  â€¢ Cosine Distance: {cosine_distance:.4f}\")\n",
    "        print(f\"  â€¢ Cosine Similarity: {similarity:.4f}\")\n",
    "\n",
    "        # Interpretation\n",
    "        if similarity > 0.8:\n",
    "            interpretation = \"Very Similar - High semantic overlap\"\n",
    "            icon = \"ğŸŸ¢\"\n",
    "        elif similarity > 0.6:\n",
    "            interpretation = \"Similar - Moderate semantic overlap\"\n",
    "            icon = \"ğŸŸ¡\"\n",
    "        elif similarity > 0.4:\n",
    "            interpretation = \"Somewhat Similar - Low semantic overlap\"\n",
    "            icon = \"ğŸŸ¡\"\n",
    "        else:\n",
    "            interpretation = \"Different - Minimal semantic overlap\"\n",
    "            icon = \"ğŸ”´\"\n",
    "\n",
    "        print(f\"  â€¢ Interpretation: {icon} {interpretation}\")\n",
    "\n",
    "        # Use case analysis\n",
    "        print(f\"\\nğŸ’¼ Use Cases:\")\n",
    "        if similarity > 0.7:\n",
    "            print(f\"  â€¢ Document Clustering: Good candidates for grouping\")\n",
    "            print(f\"  â€¢ Precedent Matching: Strong legal precedent relationship\")\n",
    "            print(f\"  â€¢ Content Recommendation: Highly relevant for cross-referencing\")\n",
    "        elif similarity > 0.5:\n",
    "            print(f\"  â€¢ Related Documents: Moderate relevance for research\")\n",
    "            print(f\"  â€¢ Topic Clustering: Suitable for broader topic grouping\")\n",
    "        else:\n",
    "            print(f\"  â€¢ Diverse Content: Documents cover different legal areas\")\n",
    "            print(f\"  â€¢ Portfolio Analysis: Shows breadth of legal domains\")\n",
    "\n",
    "        return {\n",
    "            'doc1_id': doc1_id,\n",
    "            'doc2_id': doc2_id,\n",
    "            'cosine_distance': cosine_distance,\n",
    "            'cosine_similarity': similarity,\n",
    "            'interpretation': interpretation\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ML.DISTANCE analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def ml_distance_query_document_similarity(query_text, document_ids):\n",
    "    \"\"\"\n",
    "    Use ML.DISTANCE to compare search query embeddings with found document embeddings.\n",
    "\n",
    "    Args:\n",
    "        query_text: Original search query text\n",
    "        document_ids: List of document IDs found by VECTOR_SEARCH\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with query-document similarity results\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” ML.DISTANCE Query-Document Similarity Analysis\")\n",
    "    print(f\"Query: '{query_text}'\")\n",
    "    print(f\"Found Documents: {len(document_ids)}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    try:\n",
    "        # Build query to compare query embedding with document embeddings\n",
    "        doc_list = \"', '\".join(document_ids)\n",
    "        query = f\"\"\"\n",
    "        WITH query_embedding AS (\n",
    "          SELECT\n",
    "            ml_generate_embedding_result AS query_emb\n",
    "          FROM ML.GENERATE_EMBEDDING(\n",
    "            MODEL `{config['project']['id']}.ai_models.text_embedding`,\n",
    "            (SELECT '{query_text}' AS content)\n",
    "          )\n",
    "        )\n",
    "        SELECT\n",
    "          doc.document_id,\n",
    "          ML.DISTANCE(\n",
    "            doc.embedding,\n",
    "            query_emb,\n",
    "            'COSINE'\n",
    "          ) AS cosine_distance,\n",
    "          (1 - ML.DISTANCE(\n",
    "            doc.embedding,\n",
    "            query_emb,\n",
    "            'COSINE'\n",
    "          )) AS cosine_similarity\n",
    "        FROM `{config['project']['id']}.legal_ai_platform_vector_indexes.document_embeddings` doc\n",
    "        CROSS JOIN query_embedding\n",
    "        WHERE doc.document_id IN ('{doc_list}')\n",
    "        ORDER BY cosine_similarity DESC\n",
    "        \"\"\"\n",
    "\n",
    "        result = client.query(query)\n",
    "        similarities = []\n",
    "\n",
    "        print(f\"ğŸ“Š Query-Document Similarity Rankings:\")\n",
    "        print(f\"{'Rank':<4} {'Document ID':<15} {'Similarity':<12} {'Distance':<12} {'Match Quality'}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        for i, row in enumerate(result, 1):\n",
    "            similarity = row.cosine_similarity\n",
    "            distance = row.cosine_distance\n",
    "\n",
    "            # Categorize match quality\n",
    "            if similarity > 0.8:\n",
    "                match_quality = \"ğŸŸ¢ Excellent Match\"\n",
    "            elif similarity > 0.7:\n",
    "                match_quality = \"ğŸŸ¢ Good Match\"\n",
    "            elif similarity > 0.6:\n",
    "                match_quality = \"ğŸŸ¡ Fair Match\"\n",
    "            elif similarity > 0.5:\n",
    "                match_quality = \"ğŸŸ  Poor Match\"\n",
    "            else:\n",
    "                match_quality = \"ğŸ”´ No Match\"\n",
    "\n",
    "            similarities.append({\n",
    "                'document_id': row.document_id,\n",
    "                'cosine_distance': distance,\n",
    "                'cosine_similarity': similarity,\n",
    "                'match_quality': match_quality,\n",
    "                'rank': i\n",
    "            })\n",
    "\n",
    "            print(f\"{i:<4} {row.document_id:<15} {similarity:<12.4f} {distance:<12.4f} {match_quality}\")\n",
    "\n",
    "        # Analysis of search quality\n",
    "        if similarities:\n",
    "            avg_similarity = sum(s['cosine_similarity'] for s in similarities) / len(similarities)\n",
    "            max_similarity = max(s['cosine_similarity'] for s in similarities)\n",
    "            min_similarity = min(s['cosine_similarity'] for s in similarities)\n",
    "\n",
    "            excellent_matches = len([s for s in similarities if s['cosine_similarity'] > 0.8])\n",
    "            good_matches = len([s for s in similarities if s['cosine_similarity'] > 0.7])\n",
    "\n",
    "            print(f\"\\nğŸ“ˆ Search Quality Analysis:\")\n",
    "            print(f\"  â€¢ Average Query-Document Similarity: {avg_similarity:.4f}\")\n",
    "            print(f\"  â€¢ Best Match: {max_similarity:.4f}\")\n",
    "            print(f\"  â€¢ Worst Match: {min_similarity:.4f}\")\n",
    "            print(f\"  â€¢ Similarity Range: {max_similarity - min_similarity:.4f}\")\n",
    "            print(f\"  â€¢ Excellent Matches (>0.8): {excellent_matches}/{len(similarities)}\")\n",
    "            print(f\"  â€¢ Good Matches (>0.7): {good_matches}/{len(similarities)}\")\n",
    "\n",
    "            # Search effectiveness assessment\n",
    "            if avg_similarity > 0.7:\n",
    "                effectiveness = \"ğŸŸ¢ Highly Effective\"\n",
    "            elif avg_similarity > 0.6:\n",
    "                effectiveness = \"ğŸŸ¡ Moderately Effective\"\n",
    "            elif avg_similarity > 0.5:\n",
    "                effectiveness = \"ğŸŸ  Somewhat Effective\"\n",
    "            else:\n",
    "                effectiveness = \"ğŸ”´ Ineffective\"\n",
    "\n",
    "            print(f\"  â€¢ Overall Search Effectiveness: {effectiveness}\")\n",
    "\n",
    "        return {\n",
    "            'query_text': query_text,\n",
    "            'similarities': similarities,\n",
    "            'avg_similarity': avg_similarity if similarities else 0,\n",
    "            'max_similarity': max_similarity if similarities else 0,\n",
    "            'min_similarity': min_similarity if similarities else 0,\n",
    "            'excellent_matches': excellent_matches if similarities else 0,\n",
    "            'good_matches': good_matches if similarities else 0\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Query-document similarity analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def ml_distance_document_clustering(document_ids, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Use ML.DISTANCE to cluster documents by similarity using BigQuery.\n",
    "\n",
    "    Args:\n",
    "        document_ids: List of document IDs to cluster\n",
    "        similarity_threshold: Minimum similarity for clustering\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with clustering results\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” ML.DISTANCE Document Clustering\")\n",
    "    print(f\"Documents: {len(document_ids)}\")\n",
    "    print(f\"Similarity Threshold: {similarity_threshold}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # Build query for pairwise similarity matrix\n",
    "        doc_list = \"', '\".join(document_ids)\n",
    "        query = f\"\"\"\n",
    "        WITH similarity_matrix AS (\n",
    "          SELECT\n",
    "            doc1.document_id as doc1,\n",
    "            doc2.document_id as doc2,\n",
    "            ML.DISTANCE(doc1.embedding, doc2.embedding, 'COSINE') as distance,\n",
    "            (1 - ML.DISTANCE(doc1.embedding, doc2.embedding, 'COSINE')) as similarity\n",
    "          FROM `{config['project']['id']}.legal_ai_platform_vector_indexes.document_embeddings` doc1\n",
    "          CROSS JOIN `{config['project']['id']}.legal_ai_platform_vector_indexes.document_embeddings` doc2\n",
    "          WHERE doc1.document_id IN ('{doc_list}')\n",
    "            AND doc2.document_id IN ('{doc_list}')\n",
    "            AND doc1.document_id < doc2.document_id  -- Avoid duplicates and self-comparison\n",
    "        )\n",
    "        SELECT\n",
    "          doc1,\n",
    "          doc2,\n",
    "          distance,\n",
    "          similarity,\n",
    "          CASE\n",
    "            WHEN similarity >= {similarity_threshold} THEN 'Similar'\n",
    "            ELSE 'Different'\n",
    "          END as cluster_status\n",
    "        FROM similarity_matrix\n",
    "        ORDER BY similarity DESC\n",
    "        \"\"\"\n",
    "\n",
    "        result = client.query(query)\n",
    "        clusters = []\n",
    "        similar_pairs = 0\n",
    "\n",
    "        print(f\"ğŸ“Š Document Similarity Matrix:\")\n",
    "        print(f\"{'Doc 1':<15} {'Doc 2':<15} {'Similarity':<12} {'Distance':<12} {'Status'}\")\n",
    "        print(\"-\" * 75)\n",
    "\n",
    "        for row in result:\n",
    "            clusters.append({\n",
    "                'doc1': row.doc1,\n",
    "                'doc2': row.doc2,\n",
    "                'distance': row.distance,\n",
    "                'similarity': row.similarity,\n",
    "                'cluster_status': row.cluster_status\n",
    "            })\n",
    "\n",
    "            status_icon = \"ğŸŸ¢\" if row.similarity >= similarity_threshold else \"ğŸ”´\"\n",
    "            if row.similarity >= similarity_threshold:\n",
    "                similar_pairs += 1\n",
    "\n",
    "            print(f\"{row.doc1:<15} {row.doc2:<15} {row.similarity:<12.4f} {row.distance:<12.4f} {status_icon} {row.cluster_status}\")\n",
    "\n",
    "        # Clustering analysis\n",
    "        total_pairs = len(clusters)\n",
    "        similar_percentage = (similar_pairs / total_pairs * 100) if total_pairs > 0 else 0\n",
    "\n",
    "        print(f\"\\nğŸ“ˆ Clustering Analysis:\")\n",
    "        print(f\"  â€¢ Total Document Pairs: {total_pairs}\")\n",
    "        print(f\"  â€¢ Similar Pairs (â‰¥{similarity_threshold}): {similar_pairs}\")\n",
    "        print(f\"  â€¢ Similarity Percentage: {similar_percentage:.1f}%\")\n",
    "\n",
    "        # Find most similar and least similar pairs\n",
    "        if clusters:\n",
    "            most_similar = max(clusters, key=lambda x: x['similarity'])\n",
    "            least_similar = min(clusters, key=lambda x: x['similarity'])\n",
    "\n",
    "            print(f\"  â€¢ Most Similar: {most_similar['doc1']} â†” {most_similar['doc2']} ({most_similar['similarity']:.4f})\")\n",
    "            print(f\"  â€¢ Least Similar: {least_similar['doc1']} â†” {least_similar['doc2']} ({least_similar['similarity']:.4f})\")\n",
    "\n",
    "        return {\n",
    "            'clusters': clusters,\n",
    "            'similar_pairs': similar_pairs,\n",
    "            'total_pairs': total_pairs,\n",
    "            'similarity_percentage': similar_percentage,\n",
    "            'threshold': similarity_threshold\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Document clustering failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run simplified VECTOR_SEARCH analysis\n",
    "if 'search_result' in locals() and isinstance(search_result, dict) and 'results' in search_result:\n",
    "    df_search = analyze_vector_search_results(search_result)\n",
    "\n",
    "    # Show query comparison\n",
    "    print(\"\\nğŸ“Š Query Performance Summary:\")\n",
    "    for query, result in search_results.items():\n",
    "        if 'results' in result and result['results']:\n",
    "            avg_sim = sum(r['similarity_score'] for r in result['results']) / len(result['results'])\n",
    "            print(f\"  â€¢ '{query}': avg similarity {avg_sim:.3f}\")\n",
    "\n",
    "    # Demonstrate ML.DISTANCE Query-Document Similarity Analysis\n",
    "    if len(df_search) >= 2:\n",
    "        print(\"\\nğŸ” ML.DISTANCE Query-Document Similarity Analysis:\")\n",
    "        found_docs = df_search['document_id'].tolist()\n",
    "        query_doc_similarity = ml_distance_query_document_similarity(search_result['query_text'], found_docs)\n",
    "\n",
    "        if query_doc_similarity:\n",
    "            print(f\"\\nâœ… ML.DISTANCE query-document analysis completed\")\n",
    "            print(f\"Query '{query_doc_similarity['query_text']}' vs {len(query_doc_similarity['similarities'])} documents\")\n",
    "            print(f\"Average similarity: {query_doc_similarity['avg_similarity']:.3f}\")\n",
    "            print(f\"Best match: {query_doc_similarity['max_similarity']:.3f}\")\n",
    "            print(f\"Excellent matches: {query_doc_similarity['excellent_matches']}/{len(query_doc_similarity['similarities'])}\")\n",
    "\n",
    "        # Also demonstrate pairwise document comparison\n",
    "        print(f\"\\nğŸ” ML.DISTANCE Pairwise Document Comparison:\")\n",
    "        top_docs = df_search.head(2)['document_id'].tolist()\n",
    "        distance_result = vector_distance_analysis(top_docs[0], top_docs[1])\n",
    "\n",
    "        if distance_result:\n",
    "            print(f\"âœ… Pairwise comparison: {top_docs[0]} â†” {top_docs[1]} = {distance_result['cosine_similarity']:.3f} similarity\")\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for analysis. Please run vector_search() first.\")"
   ],
   "id": "dd9f58fc-c5bb-46f5-b124-95565415358f"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
